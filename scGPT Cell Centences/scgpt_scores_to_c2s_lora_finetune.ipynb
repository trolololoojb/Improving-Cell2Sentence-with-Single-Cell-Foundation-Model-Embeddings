{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# scGPT → Scores → CellSentences → C2S LoRA Fine-tuning (2 Varianten)\n",
        "\n",
        "Dieses Notebook macht (rechen-sparsam) genau das:\n",
        "\n",
        "1. Lädt die **Top‑K Gene + Werte** pro Zelle aus `processed/scgpt_inputs_topk.npz` (aus dem Preprocessing-Notebook).\n",
        "2. Lädt **scGPT**, berechnet pro Zelle `mlm_output` für diese Top‑K Positionen.\n",
        "3. Baut daraus **scGPT-CellSentences** (Gene nach `mlm_output` sortiert).\n",
        "4. Fine-tuned **C2S** (als Causal LM) mit **LoRA** für *Cell-Label-Prediction* in zwei Varianten:\n",
        "   - **Baseline:** CellSentences nach Expression-Ranking\n",
        "   - **scGPT:** CellSentences nach scGPT-Score-Ranking\n",
        "\n",
        "Am Ende werden beide Modelle auf dem **gleichen Testsplit** evaluiert.\n",
        "\n",
        "> Hinweis: Modellpfade/Label-Spalte können je nach Setup variieren. Die Parameter unten sind bewusst als Variablen gesetzt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE: cpu\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 0) Imports\n",
        "# ======================\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import scanpy as sc\n",
        "from scipy import sparse\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# HF / LoRA\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DEVICE:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Pfade & Parameter\n",
        "\n",
        "Passe diese Variablen an dein Setup an.\n",
        "\n",
        "- `PROCESSED_DIR` kommt aus dem Preprocessing-Notebook.\n",
        "- `SCGPT_MODEL_DIR` muss auf einen lokalen scGPT-Checkpoint zeigen.\n",
        "- `C2S_MODEL_NAME_OR_PATH` ist das Basismodell für C2S (Causal LM).  \n",
        "  Wenn du ein lokales Modell nutzt, gib den Pfad an. Wenn es ein HF Hub Modell ist, den Namen.\n",
        "\n",
        "**Compute-Tipp:** halte `K_TOP` klein (256/512) und LoRA rank niedrig (8/16).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 1) Konfiguration\n",
        "# ======================\n",
        "PROCESSED_DIR = \"processed\"\n",
        "\n",
        "# aus Preprocessing\n",
        "NPZ_TOPK = os.path.join(PROCESSED_DIR, \"scgpt_inputs_topk.npz\")\n",
        "C2S_SENT_BASELINE = os.path.join(PROCESSED_DIR, \"c2s_sentences_expression.txt\")\n",
        "ADATA_HVG = os.path.join(PROCESSED_DIR, \"processed_adata_hvg.h5ad\")\n",
        "SPLITS_CSV = os.path.join(PROCESSED_DIR, \"splits.csv\")\n",
        "\n",
        "# scGPT: lokaler Modellordner (Checkpoint + ggf. vocab config)\n",
        "SCGPT_MODEL_DIR = \"path/to/scgpt_checkpoint_dir\"\n",
        "\n",
        "# C2S Basismodell (Causal LM) – muss ein HF-kompatibles AutoModelForCausalLM sein\n",
        "C2S_MODEL_NAME_OR_PATH = \"path/to/c2s_base_model\"\n",
        "\n",
        "# Training\n",
        "K_TOP = 512              # muss zum Preprocessing passen\n",
        "MAX_LEN = 768            # Prompt+Answer Gesamtlänge (ggf. anpassen)\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 16\n",
        "EPOCHS = 2\n",
        "LR = 2e-4\n",
        "WARMUP_RATIO = 0.03\n",
        "\n",
        "# LoRA (sparsam)\n",
        "LORA_R = 8 # Rang der Low-Rank-Approximation\n",
        "LORA_ALPHA = 16 # Skalierungsfaktor für die LoRA-Updates\n",
        "LORA_DROPOUT = 0.05 # Dropout für die LoRA-Adapter, um Überanpassung zu vermeiden\n",
        "\n",
        "OUT_BASELINE = os.path.join(PROCESSED_DIR, \"c2s_lora_baseline\")\n",
        "OUT_SCGPT = os.path.join(PROCESSED_DIR, \"c2s_lora_scgpt\")\n",
        "\n",
        "os.makedirs(OUT_BASELINE, exist_ok=True)\n",
        "os.makedirs(OUT_SCGPT, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Labels finden (aus `.h5ad`)\n",
        "\n",
        "Wir laden die AnnData und versuchen automatisch eine sinnvolle Label-Spalte in `adata.obs` zu finden.  \n",
        "Wenn der Automatismus die falsche Spalte wählt, setze `LABEL_COL` manuell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 2) Lade AnnData und finde Label-Spalte\n",
        "# ======================\n",
        "adata = sc.read_h5ad(ADATA_HVG)\n",
        "splits = pd.read_csv(SPLITS_CSV)\n",
        "\n",
        "candidate_label_cols = [\n",
        "    \"cell_type\", \"celltype\", \"cell_type_original\", \"celltype_major\",\n",
        "    \"celltype_minor\", \"cell_type_fine\", \"annotation\", \"labels\", \"label\",\n",
        "]\n",
        "\n",
        "LABEL_COL = None\n",
        "for c in candidate_label_cols:\n",
        "    if c in adata.obs.columns:\n",
        "        LABEL_COL = c\n",
        "        break\n",
        "\n",
        "if LABEL_COL is None:\n",
        "    for c in adata.obs.columns:\n",
        "        if (pd.api.types.is_categorical_dtype(adata.obs[c]) or adata.obs[c].dtype == object):\n",
        "            nunique = adata.obs[c].nunique()\n",
        "            if 2 <= nunique <= 200:\n",
        "                LABEL_COL = c\n",
        "                break\n",
        "\n",
        "print(\"Chosen LABEL_COL:\", LABEL_COL)\n",
        "if LABEL_COL is None:\n",
        "    raise ValueError(\"Keine Label-Spalte gefunden. Bitte LABEL_COL manuell setzen.\")\n",
        "\n",
        "if \"cell_id\" not in adata.obs.columns:\n",
        "    adata.obs[\"cell_id\"] = adata.obs_names.astype(str)\n",
        "\n",
        "obs = adata.obs[[\"cell_id\", LABEL_COL]].copy()\n",
        "obs = obs.merge(splits, on=\"cell_id\", how=\"inner\")\n",
        "print(obs[\"split\"].value_counts())\n",
        "print(\"n_labels:\", obs[LABEL_COL].nunique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Baseline-Sentences laden\n",
        "Format: `cell_id<TAB>GENE1 GENE2 ...`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def load_sentence_tsv(path: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            cid, sent = line.split(\"\\t\", 1)\n",
        "            rows.append((cid, sent))\n",
        "    return pd.DataFrame(rows, columns=[\"cell_id\", \"sentence\"])\n",
        "\n",
        "df_base = load_sentence_tsv(C2S_SENT_BASELINE)\n",
        "print(df_base.head())\n",
        "print(\"n_sentences:\", len(df_base))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) scGPT: `mlm_output` berechnen und scGPT-Sentences bauen\n",
        "\n",
        "Wir nutzen **dieselben Top‑K Gene** wie die Baseline (aus `scgpt_inputs_topk.npz`), berechnen `mlm_output` und sortieren danach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 4A) Lade Top-K Gene/Werte pro Zelle\n",
        "# ======================\n",
        "npz = np.load(NPZ_TOPK, allow_pickle=True)\n",
        "cell_ids = npz[\"cell_id\"]\n",
        "topk_gene_symbols = npz[\"topk_gene_symbols\"]   # (n_cells, K_TOP) strings\n",
        "topk_values = npz[\"topk_values\"]               # (n_cells, K_TOP) floats\n",
        "\n",
        "assert topk_gene_symbols.shape[1] == K_TOP\n",
        "assert topk_values.shape[1] == K_TOP\n",
        "\n",
        "print(\"TopK arrays:\", topk_gene_symbols.shape, topk_values.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 4B) scGPT laden\n",
        "# ======================\n",
        "# Erwartung: scgpt ist installiert (pip install scgpt)\n",
        "# und SCGPT_MODEL_DIR enthält vocab.json, config.json und model weights.\n",
        "\n",
        "from scgpt.model import TransformerModel\n",
        "from scgpt.tokenizer import GeneVocab\n",
        "\n",
        "vocab_path = os.path.join(SCGPT_MODEL_DIR, \"vocab.json\")\n",
        "if not os.path.exists(vocab_path):\n",
        "    raise FileNotFoundError(f\"vocab.json nicht gefunden: {vocab_path}\")\n",
        "vocab = GeneVocab.from_file(vocab_path)\n",
        "\n",
        "config_path = os.path.join(SCGPT_MODEL_DIR, \"config.json\")\n",
        "if not os.path.exists(config_path):\n",
        "    raise FileNotFoundError(\n",
        "        f\"config.json nicht gefunden: {config_path}. \"\n",
        "        \"Wenn dein scGPT-Checkpoint anders strukturiert ist, passe den Loader hier an.\"\n",
        "    )\n",
        "\n",
        "with open(config_path, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "PAD_ID = vocab[\"<pad>\"] if \"<pad>\" in vocab else 0\n",
        "\n",
        "model = TransformerModel(\n",
        "    ntokens=len(vocab),\n",
        "    d_model=cfg.get(\"d_model\", 512),\n",
        "    nhead=cfg.get(\"nhead\", 8),\n",
        "    d_hid=cfg.get(\"d_hid\", 512),\n",
        "    nlayers=cfg.get(\"nlayers\", 12),\n",
        "    dropout=0.0,\n",
        "    pad_token_id=PAD_ID,\n",
        "    do_mvc=cfg.get(\"do_mvc\", False),\n",
        "    do_dab=cfg.get(\"do_dab\", False),\n",
        "    use_batch_labels=cfg.get(\"use_batch_labels\", False),\n",
        "    explicit_zero_prob=cfg.get(\"explicit_zero_prob\", False),\n",
        ")\n",
        "\n",
        "ckpt_path = os.path.join(SCGPT_MODEL_DIR, \"model.pt\")\n",
        "if not os.path.exists(ckpt_path):\n",
        "    ckpt_path = os.path.join(SCGPT_MODEL_DIR, \"best_model.pt\")\n",
        "\n",
        "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"model_state_dict\" in state:\n",
        "    model.load_state_dict(state[\"model_state_dict\"], strict=False)\n",
        "else:\n",
        "    model.load_state_dict(state, strict=False)\n",
        "\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "print(\"Loaded scGPT.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 4C) Batch inference -> mlm_output\n",
        "# ======================\n",
        "UNK_ID = vocab[\"<unk>\"] if \"<unk>\" in vocab else PAD_ID\n",
        "\n",
        "def genes_to_ids(gene_sym_row):\n",
        "    ids = []\n",
        "    for g in gene_sym_row:\n",
        "        g = str(g)\n",
        "        if g in vocab:\n",
        "            ids.append(vocab[g])\n",
        "        else:\n",
        "            ids.append(UNK_ID)\n",
        "    return np.array(ids, dtype=np.int64)\n",
        "\n",
        "def scgpt_mlm_scores_batch(gene_syms_batch, values_batch):\n",
        "    src = np.stack([genes_to_ids(row) for row in gene_syms_batch], axis=0)  # (B,K)\n",
        "    vals = values_batch.astype(np.float32)\n",
        "\n",
        "    src_t = torch.from_numpy(src).to(DEVICE)\n",
        "    vals_t = torch.from_numpy(vals).to(DEVICE)\n",
        "\n",
        "    pad_mask = torch.zeros(src_t.shape, dtype=torch.bool, device=DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(\n",
        "            src=src_t,\n",
        "            values=vals_t,\n",
        "            src_key_padding_mask=pad_mask,\n",
        "            output_hidden_states=False,\n",
        "            CLS=False,\n",
        "        )\n",
        "        mlm = out.get(\"mlm_output\", None)\n",
        "        if mlm is None:\n",
        "            raise KeyError(\"mlm_output nicht im scGPT output. Prüfe scGPT-Version/Config.\")\n",
        "        if mlm.ndim == 3 and mlm.shape[-1] == 1:\n",
        "            mlm = mlm.squeeze(-1)\n",
        "        mlm = mlm.detach().float().cpu().numpy()\n",
        "    return mlm  # (B,K)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 4D) Für alle Zellen scGPT-Scores berechnen und Sentence bauen\n",
        "# ======================\n",
        "B = 32 if DEVICE == \"cuda\" else 8\n",
        "all_scores = np.zeros((len(cell_ids), K_TOP), dtype=np.float32)\n",
        "\n",
        "for start in tqdm(range(0, len(cell_ids), B)):\n",
        "    end = min(len(cell_ids), start + B)\n",
        "    scores = scgpt_mlm_scores_batch(topk_gene_symbols[start:end], topk_values[start:end])\n",
        "    all_scores[start:end] = scores\n",
        "\n",
        "scgpt_sentences = []\n",
        "for i in range(len(cell_ids)):\n",
        "    scores = all_scores[i]\n",
        "    idx = np.argsort(-scores)[:K_TOP]\n",
        "    genes_sorted = topk_gene_symbols[i, idx]\n",
        "    scgpt_sentences.append(\" \".join(map(str, genes_sorted)))\n",
        "\n",
        "df_scgpt = pd.DataFrame({\"cell_id\": cell_ids, \"sentence\": scgpt_sentences})\n",
        "scgpt_sent_path = os.path.join(PROCESSED_DIR, \"c2s_sentences_scgpt_mlm.txt\")\n",
        "with open(scgpt_sent_path, \"w\") as f:\n",
        "    for cid, sent in zip(df_scgpt[\"cell_id\"].values, df_scgpt[\"sentence\"].values):\n",
        "        f.write(f\"{cid}\\t{sent}\\n\")\n",
        "\n",
        "print(\"Wrote:\", scgpt_sent_path)\n",
        "df_scgpt.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Supervised Fine-tuning Daten bauen (Prompt → Label)\n",
        "\n",
        "Wir trainieren ein Causal LM so, dass es nach `Answer:` das Label ausgibt.\n",
        "Loss wird nur auf den Answer-Teil gerechnet (Prompt-Masking).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"Cell: {sentence}\\nTask: predict cell type.\\nAnswer:\"\n",
        "\n",
        "def build_supervised_df(df_sent: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = obs.merge(df_sent, on=\"cell_id\", how=\"inner\")\n",
        "    df[\"prompt\"] = df[\"sentence\"].apply(lambda s: PROMPT_TEMPLATE.format(sentence=s))\n",
        "    df[\"answer\"] = df[LABEL_COL].astype(str)\n",
        "    df[\"text_full\"] = df[\"prompt\"] + \" \" + df[\"answer\"]\n",
        "    return df[[\"cell_id\", \"split\", \"prompt\", \"answer\", \"text_full\"]]\n",
        "\n",
        "df_train_base = build_supervised_df(df_base)\n",
        "df_train_scgpt = build_supervised_df(df_scgpt)\n",
        "\n",
        "print(df_train_base.head(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Tokenisierung + Collator mit Prompt-Masking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "class SFTDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer, max_len: int):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        full = row[\"text_full\"]\n",
        "        prompt = row[\"prompt\"]\n",
        "\n",
        "        enc_full = self.tok(full, truncation=True, max_length=self.max_len, padding=False)\n",
        "        enc_prompt = self.tok(prompt, truncation=True, max_length=self.max_len, padding=False)\n",
        "\n",
        "        input_ids = enc_full[\"input_ids\"]\n",
        "        attn = enc_full[\"attention_mask\"]\n",
        "\n",
        "        labels = np.array(input_ids, dtype=np.int64)\n",
        "        prompt_len = len(enc_prompt[\"input_ids\"])\n",
        "        labels[:prompt_len] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "            \"answer\": row[\"answer\"],\n",
        "        }\n",
        "\n",
        "def make_collate_fn(tokenizer):\n",
        "    def collate(batch):\n",
        "        max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
        "        input_ids = []\n",
        "        attention_mask = []\n",
        "        labels = []\n",
        "        answers = []\n",
        "        for x in batch:\n",
        "            pad = max_len - len(x[\"input_ids\"])\n",
        "            input_ids.append(torch.cat([x[\"input_ids\"], torch.full((pad,), tokenizer.pad_token_id, dtype=torch.long)]))\n",
        "            attention_mask.append(torch.cat([x[\"attention_mask\"], torch.zeros((pad,), dtype=torch.long)]))\n",
        "            labels.append(torch.cat([x[\"labels\"], torch.full((pad,), -100, dtype=torch.long)]))\n",
        "            answers.append(x[\"answer\"])\n",
        "        return {\n",
        "            \"input_ids\": torch.stack(input_ids),\n",
        "            \"attention_mask\": torch.stack(attention_mask),\n",
        "            \"labels\": torch.stack(labels),\n",
        "            \"answers\": answers,\n",
        "        }\n",
        "    return collate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) C2S Basismodell laden + LoRA anhängen\n",
        "\n",
        "Falls dein Basismodell keine `q_proj/v_proj` Module hat (z. B. GPT2), musst du `target_modules` anpassen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(C2S_MODEL_NAME_OR_PATH, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def make_lora_model():\n",
        "    base = AutoModelForCausalLM.from_pretrained(\n",
        "        C2S_MODEL_NAME_OR_PATH,\n",
        "        torch_dtype=torch.float16 if (DEVICE == \"cuda\") else torch.float32,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    target_modules = [\"q_proj\", \"v_proj\"]  # ggf. anpassen!\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "    model = get_peft_model(base, lora_cfg)\n",
        "    model.print_trainable_parameters()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Training-Funktion (gleiche Settings für beide Varianten)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def train_one(df_all: pd.DataFrame, out_dir: str):\n",
        "    df_tr = df_all[df_all[\"split\"] == \"train\"].copy()\n",
        "    df_va = df_all[df_all[\"split\"] == \"val\"].copy()\n",
        "\n",
        "    ds_tr = SFTDataset(df_tr, tokenizer, MAX_LEN)\n",
        "    ds_va = SFTDataset(df_va, tokenizer, MAX_LEN)\n",
        "\n",
        "    model = make_lora_model()\n",
        "    collate_fn = make_collate_fn(tokenizer)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "        fp16=(DEVICE == \"cuda\"),\n",
        "        report_to=\"none\",\n",
        "        seed=SEED,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=ds_tr,\n",
        "        eval_dataset=ds_va,\n",
        "        data_collator=collate_fn,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(out_dir)\n",
        "    tokenizer.save_pretrained(out_dir)\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Train Baseline-LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "trainer_base = train_one(df_train_base, OUT_BASELINE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Train scGPT-LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "trainer_scgpt = train_one(df_train_scgpt, OUT_SCGPT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Evaluation: Label generieren auf Testset\n",
        "\n",
        "Wir generieren kurz nach `Answer:` und vergleichen mit Ground Truth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_labels(model_dir: str, df_all: pd.DataFrame, max_new_tokens: int = 12):\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        torch_dtype=torch.float16 if (DEVICE == \"cuda\") else torch.float32,\n",
        "    ).to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    df_te = df_all[df_all[\"split\"] == \"test\"].copy().reset_index(drop=True)\n",
        "\n",
        "    preds = []\n",
        "    trues = df_te[\"answer\"].tolist()\n",
        "\n",
        "    for i in tqdm(range(len(df_te))):\n",
        "        prompt = df_te.loc[i, \"prompt\"]\n",
        "        enc = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(DEVICE)\n",
        "        gen = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            num_beams=1,\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "        )\n",
        "        out = tok.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "        pred = out.split(\"Answer:\", 1)[1].strip() if \"Answer:\" in out else out.strip()\n",
        "        pred = pred.split(\"\\n\")[0].strip()\n",
        "        pred = pred.split(\".\")[0].split(\",\")[0].strip()\n",
        "        preds.append(pred)\n",
        "\n",
        "    return trues, preds\n",
        "\n",
        "def evaluate(trues, preds):\n",
        "    return accuracy_score(trues, preds), f1_score(trues, preds, average=\"macro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "y_true_b, y_pred_b = predict_labels(OUT_BASELINE, df_train_base)\n",
        "acc_b, f1_b = evaluate(y_true_b, y_pred_b)\n",
        "print(\"BASELINE  acc:\", acc_b, \"macroF1:\", f1_b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "y_true_s, y_pred_s = predict_labels(OUT_SCGPT, df_train_scgpt)\n",
        "acc_s, f1_s = evaluate(y_true_s, y_pred_s)\n",
        "print(\"scGPT     acc:\", acc_s, \"macroF1:\", f1_s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting (kurz)\n",
        "\n",
        "**OOM / zu langsam**\n",
        "- `K_TOP=256`\n",
        "- `MAX_LEN` runter\n",
        "- `BATCH_SIZE=1`, `GRAD_ACCUM` hoch\n",
        "- `LORA_R=4`\n",
        "\n",
        "**LoRA target_modules passt nicht**\n",
        "- Für GPT2-artige Modelle: oft `\"c_attn\"` / `\"c_proj\"` statt `q_proj/v_proj`.\n",
        "- Einfach `print(model)` und nach Modulnamen suchen.\n",
        "\n",
        "**Labels passen nicht**\n",
        "- `LABEL_COL` manuell setzen.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

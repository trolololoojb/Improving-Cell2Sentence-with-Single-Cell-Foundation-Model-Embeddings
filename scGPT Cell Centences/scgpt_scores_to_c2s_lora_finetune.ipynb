{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# scGPT → Scores → CellSentences → C2S LoRA Fine-tuning (2 Varianten)\n",
        "\n",
        "Dieses Notebook macht (rechen-sparsam) genau das:\n",
        "\n",
        "1. Lädt die **Top‑K Gene + Werte** pro Zelle aus `processed/scgpt_inputs_topk.npz` (aus dem Preprocessing-Notebook).\n",
        "2. Lädt **scGPT**, berechnet pro Zelle `mlm_output` für diese Top‑K Positionen.\n",
        "3. Baut daraus **scGPT-CellSentences** (Gene nach `mlm_output` sortiert).\n",
        "4. Fine-tuned **C2S** (als Causal LM) mit **LoRA** für *Cell-Label-Prediction* in zwei Varianten:\n",
        "   - **Baseline:** CellSentences nach Expression-Ranking\n",
        "   - **scGPT:** CellSentences nach scGPT-Score-Ranking\n",
        "\n",
        "Am Ende werden beide Modelle auf dem **gleichen Testsplit** evaluiert.\n",
        "\n",
        "> Hinweis: Modellpfade/Label-Spalte können je nach Setup variieren. Die Parameter unten sind bewusst als Variablen gesetzt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "40bfe8c8",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/j-u-b/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Disabling PyTorch because PyTorch >= 2.4 is required but found 2.3.1+cu121\n",
            "PyTorch was not found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'LRScheduler' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     AutoTokenizer,\n\u001b[32m     19\u001b[39m     AutoModelForCausalLM,\n\u001b[32m     20\u001b[39m     TrainingArguments,\n\u001b[32m     21\u001b[39m     Trainer,\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[32m     26\u001b[39m SEED = \u001b[32m42\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1229\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/transformers/utils/import_utils.py:2044\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2042\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2043\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2044\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2045\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2046\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2047\u001b[39m         \u001b[38;5;66;03m# V5: If trying to import a *TokenizerFast symbol, transparently fall back to the\u001b[39;00m\n\u001b[32m   2048\u001b[39m         \u001b[38;5;66;03m# non-Fast symbol from the same module when available. This lets us keep only one\u001b[39;00m\n\u001b[32m   2049\u001b[39m         \u001b[38;5;66;03m# backend tokenizer class while preserving legacy public names.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/transformers/utils/import_utils.py:2238\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2236\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/transformers/utils/import_utils.py:2236\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2234\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2235\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2236\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2237\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2238\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/importlib/__init__.py:126\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m    124\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    125\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/transformers/training_args.py:71\u001b[39m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState, PartialState\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistributedType\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrainer_pt_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AcceleratorConfig\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available(\u001b[33m\"\u001b[39m\u001b[33m1.10.1\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallelism_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParallelismConfig\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:1270\u001b[39m\n\u001b[32m   1266\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mfloat\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1267\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1270\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mLayerWiseDummyScheduler\u001b[39;00m(\u001b[43mLRScheduler\u001b[49m):\n\u001b[32m   1271\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1272\u001b[39m \u001b[33;03m    For Layer-wise optimizers such as GaLoRE optimizer, the optimization and scheduling step\u001b[39;00m\n\u001b[32m   1273\u001b[39m \u001b[33;03m    are already done through the post gradient hooks. Therefore\u001b[39;00m\n\u001b[32m   1274\u001b[39m \u001b[33;03m    the trick is to create a dummy scheduler that can take arbitrary\u001b[39;00m\n\u001b[32m   1275\u001b[39m \u001b[33;03m    args and kwargs and return a no-op during training.\u001b[39;00m\n\u001b[32m   1276\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1278\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n",
            "\u001b[31mNameError\u001b[39m: name 'LRScheduler' is not defined"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 0) Imports\n",
        "# ======================\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import scanpy as sc\n",
        "from scipy import sparse\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# HF / LoRA\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"DEVICE:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359a9a59",
      "metadata": {},
      "source": [
        "## 1) Pfade & Parameter\n",
        "\n",
        "Passe diese Variablen an dein Setup an.\n",
        "\n",
        "- `PROCESSED_DIR` kommt aus dem Preprocessing-Notebook.\n",
        "- `SCGPT_MODEL_DIR` muss auf einen lokalen scGPT-Checkpoint zeigen.\n",
        "- `C2S_MODEL_NAME_OR_PATH` ist das Basismodell für C2S (Causal LM).  \n",
        "  Wenn du ein lokales Modell nutzt, gib den Pfad an. Wenn es ein HF Hub Modell ist, den Namen.\n",
        "\n",
        "**Compute-Tipp:** halte `K_TOP` klein (256/512) und LoRA rank niedrig (8/16).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94d5736b",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 1) Konfiguration\n",
        "# ======================\n",
        "PROCESSED_DIR = \"processed\"\n",
        "\n",
        "# aus Preprocessing\n",
        "NPZ_TOPK = os.path.join(PROCESSED_DIR, \"scgpt_inputs_topk.npz\")\n",
        "C2S_SENT_BASELINE = os.path.join(PROCESSED_DIR, \"c2s_sentences_expression.txt\")\n",
        "ADATA_HVG = os.path.join(PROCESSED_DIR, \"processed_adata_hvg.h5ad\")\n",
        "SPLITS_CSV = os.path.join(PROCESSED_DIR, \"splits.csv\")\n",
        "\n",
        "# scGPT: lokaler Modellordner (Checkpoint + ggf. vocab config)\n",
        "SCGPT_MODEL_DIR = \"../models/scGPT\"\n",
        "\n",
        "# C2S Basismodell (Causal LM) – muss ein HF-kompatibles AutoModelForCausalLM sein\n",
        "C2S_MODEL_NAME_OR_PATH = \"path/to/c2s_base_model\"\n",
        "\n",
        "# Training\n",
        "K_TOP = 512              # muss zum Preprocessing passen\n",
        "MAX_LEN = 768            # Prompt+Answer Gesamtlänge (ggf. anpassen)\n",
        "\n",
        "# scGPT masking (Inference)\n",
        "MLM_PROB = 0.40        # z.B. 40% der Gene maskieren\n",
        "MASK_VALUE = -1        # scGPT default mask_value\n",
        "# Falls dein Checkpoint andere Konventionen hat:\n",
        "# MASK_VALUE = 0\n",
        "MASK_N_RUNS = 1        # später ggf. 3 und mitteln\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 16\n",
        "EPOCHS = 2\n",
        "LR = 2e-4\n",
        "WARMUP_RATIO = 0.03\n",
        "\n",
        "# LoRA (sparsam)\n",
        "LORA_R = 8 # Rang der Low-Rank-Approximation\n",
        "LORA_ALPHA = 16 # Skalierungsfaktor für die LoRA-Updates\n",
        "LORA_DROPOUT = 0.05 # Dropout für die LoRA-Adapter, um Überanpassung zu vermeiden\n",
        "\n",
        "OUT_BASELINE = os.path.join(PROCESSED_DIR, \"c2s_lora_baseline\")\n",
        "OUT_SCGPT = os.path.join(PROCESSED_DIR, \"c2s_lora_scgpt\")\n",
        "\n",
        "os.makedirs(OUT_BASELINE, exist_ok=True)\n",
        "os.makedirs(OUT_SCGPT, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dac276f",
      "metadata": {},
      "source": [
        "## 2) Labels finden (aus `.h5ad`)\n",
        "\n",
        "Wir laden die AnnData und versuchen automatisch eine sinnvolle Label-Spalte in `adata.obs` zu finden.  \n",
        "Wenn der Automatismus die falsche Spalte wählt, setze `LABEL_COL` manuell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae698fcc",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chosen LABEL_COL: cell_type\n",
            "split\n",
            "train    20841\n",
            "val       4466\n",
            "test      4466\n",
            "Name: count, dtype: int64\n",
            "n_labels: 35\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 2) Lade AnnData und finde Label-Spalte\n",
        "# ======================\n",
        "adata = sc.read_h5ad(ADATA_HVG)\n",
        "splits = pd.read_csv(SPLITS_CSV)\n",
        "\n",
        "candidate_label_cols = [\n",
        "    \"cell_type\", \"celltype\", \"cell_type_original\", \"celltype_major\",\n",
        "    \"celltype_minor\", \"cell_type_fine\", \"annotation\", \"labels\", \"label\",\n",
        "]\n",
        "\n",
        "LABEL_COL = None\n",
        "for c in candidate_label_cols:\n",
        "    if c in adata.obs.columns:\n",
        "        LABEL_COL = c\n",
        "        break\n",
        "\n",
        "if LABEL_COL is None:\n",
        "    for c in adata.obs.columns:\n",
        "        if (pd.api.types.is_categorical_dtype(adata.obs[c]) or adata.obs[c].dtype == object):\n",
        "            nunique = adata.obs[c].nunique()\n",
        "            if 2 <= nunique <= 200:\n",
        "                LABEL_COL = c\n",
        "                break\n",
        "\n",
        "print(\"Chosen LABEL_COL:\", LABEL_COL)\n",
        "if LABEL_COL is None:\n",
        "    raise ValueError(\"Keine Label-Spalte gefunden. Bitte LABEL_COL manuell setzen.\")\n",
        "\n",
        "if \"cell_id\" not in adata.obs.columns:\n",
        "    adata.obs[\"cell_id\"] = adata.obs_names.astype(str)\n",
        "\n",
        "obs = adata.obs[[\"cell_id\", LABEL_COL]].copy()\n",
        "obs = obs.merge(splits, on=\"cell_id\", how=\"inner\")\n",
        "print(obs[\"split\"].value_counts())\n",
        "print(\"n_labels:\", obs[LABEL_COL].nunique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c483454",
      "metadata": {},
      "source": [
        "## 3) Baseline-Sentences laden\n",
        "Format: `cell_id<TAB>GENE1 GENE2 ...`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b2da7b8",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                         cell_id  \\\n",
            "0  Pan_T7935490_AAACCTGCAAATTGCC   \n",
            "1  Pan_T7935490_AAACGGGCATCTGGTA   \n",
            "2  Pan_T7935490_AAACGGGTCTTGCATT   \n",
            "3  Pan_T7935490_AAAGCAATCATCGCTC   \n",
            "4  Pan_T7935490_AAAGTAGCAGTCACTA   \n",
            "\n",
            "                                            sentence  \n",
            "0  HSP90AA1 FTH1 KLF6 HSPA1B MALAT1 ATF3 DUSP1 FO...  \n",
            "1  MALAT1 HSPA1A HSP90AA1 RPS27 JUNB HSPA1B CD69 ...  \n",
            "2  HSPA1A MALAT1 HSP90AA1 FOS DNAJB1 RPS27 CCL5 H...  \n",
            "3  CCL4 MALAT1 CCL4L2 JUN CCL5 FOS IFNG DNAJB1 HS...  \n",
            "4  GNLY GZMA MALAT1 CD7 CCL5 HSPA1A HSP90AA1 DNAJ...  \n",
            "n_sentences: 29773\n"
          ]
        }
      ],
      "source": [
        "def load_sentence_tsv(path: str) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            cid, sent = line.split(\"\\t\", 1)\n",
        "            rows.append((cid, sent))\n",
        "    return pd.DataFrame(rows, columns=[\"cell_id\", \"sentence\"])\n",
        "\n",
        "df_base = load_sentence_tsv(C2S_SENT_BASELINE)\n",
        "print(df_base.head())\n",
        "print(\"n_sentences:\", len(df_base))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4ce137",
      "metadata": {},
      "source": [
        "## 4) scGPT: `mlm_output` berechnen und scGPT-Sentences bauen\n",
        "\n",
        "Wir nutzen **dieselben Top‑K Gene** wie die Baseline (aus `scgpt_inputs_topk.npz`), berechnen `mlm_output` und sortieren danach.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d16103b",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TopK arrays: (29773, 512) (29773, 512)\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 4A) Lade Top-K Gene/Werte pro Zelle\n",
        "# ======================\n",
        "npz = np.load(NPZ_TOPK, allow_pickle=True)\n",
        "cell_ids = npz[\"cell_id\"]\n",
        "topk_gene_symbols = npz[\"topk_gene_symbols\"]   # (n_cells, K_TOP) strings\n",
        "topk_values = npz[\"topk_values\"]               # (n_cells, K_TOP) floats\n",
        "\n",
        "assert topk_gene_symbols.shape[1] == K_TOP\n",
        "assert topk_values.shape[1] == K_TOP\n",
        "\n",
        "print(\"TopK arrays:\", topk_gene_symbols.shape, topk_values.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "421c9696",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/j-u-b/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/scgpt/model/model.py:77: UserWarning: flash-attn is not installed, using pytorch transformer instead. Set use_fast_transformer=False to avoid this warning. Installing flash-attn is highly recommended.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded scGPT from: ../models/scGPT/best_model.pt\n",
            "Missing keys: 34 | Unexpected keys: 25\n"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 4B) scGPT laden\n",
        "# ======================\n",
        "import os\n",
        "import json\n",
        "import inspect\n",
        "import torch\n",
        "from scgpt.tokenizer import GeneVocab\n",
        "\n",
        "# je nach scgpt-Version\n",
        "try:\n",
        "    from scgpt.model import TransformerModel\n",
        "except ImportError:\n",
        "    from scgpt.models import TransformerModel\n",
        "\n",
        "# 1) Vocab\n",
        "vocab_path = os.path.join(SCGPT_MODEL_DIR, \"vocab.json\")\n",
        "if not os.path.exists(vocab_path):\n",
        "    raise FileNotFoundError(f\"vocab.json nicht gefunden: {vocab_path}\")\n",
        "vocab = GeneVocab.from_file(vocab_path)\n",
        "\n",
        "# 2) Config (config.json ODER args.json)\n",
        "config_path = os.path.join(SCGPT_MODEL_DIR, \"config.json\")\n",
        "args_path = os.path.join(SCGPT_MODEL_DIR, \"args.json\")\n",
        "\n",
        "if os.path.exists(config_path):\n",
        "    with open(config_path, \"r\") as f:\n",
        "        cfg = json.load(f)\n",
        "elif os.path.exists(args_path):\n",
        "    with open(args_path, \"r\") as f:\n",
        "        cfg = json.load(f)\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Weder config.json noch args.json gefunden in {SCGPT_MODEL_DIR}\"\n",
        "    )\n",
        "\n",
        "# Key-Mapping zwischen config.json- und args.json-Schema\n",
        "d_model = cfg.get(\"d_model\", cfg.get(\"embsize\", 512))\n",
        "nhead = cfg.get(\"nhead\", cfg.get(\"nheads\", 8))\n",
        "d_hid = cfg.get(\"d_hid\", 512)\n",
        "nlayers = cfg.get(\"nlayers\", 12)\n",
        "dropout = cfg.get(\"dropout\", 0.0)\n",
        "pad_token = cfg.get(\"pad_token\", \"<pad>\")\n",
        "pad_value = cfg.get(\"pad_value\", -2)\n",
        "do_mvc = cfg.get(\"do_mvc\", cfg.get(\"MVC\", False))\n",
        "do_dab = cfg.get(\"do_dab\", cfg.get(\"DAB\", False))\n",
        "use_batch_labels = cfg.get(\"use_batch_labels\", False)\n",
        "explicit_zero_prob = cfg.get(\"explicit_zero_prob\", False)\n",
        "use_fast_transformer = cfg.get(\"fast_transformer\", True)\n",
        "nlayers_cls = cfg.get(\"nlayers_cls\", cfg.get(\"n_layers_cls\", 3))\n",
        "n_cls = cfg.get(\"n_cls\", 1)\n",
        "\n",
        "# 3) Modell bauen (versionsrobust: nur unterstützte kwargs übergeben)\n",
        "candidate_kwargs = {\n",
        "    \"ntokens\": len(vocab),\n",
        "    \"ntoken\": len(vocab),\n",
        "    \"d_model\": d_model,\n",
        "    \"nhead\": nhead,\n",
        "    \"d_hid\": d_hid,\n",
        "    \"nlayers\": nlayers,\n",
        "    \"dropout\": dropout,\n",
        "    \"pad_token_id\": vocab[pad_token] if pad_token in vocab else 0,\n",
        "    \"pad_token\": pad_token,\n",
        "    \"pad_value\": pad_value,\n",
        "    \"do_mvc\": do_mvc,\n",
        "    \"do_dab\": do_dab,\n",
        "    \"use_batch_labels\": use_batch_labels,\n",
        "    \"explicit_zero_prob\": explicit_zero_prob,\n",
        "    \"use_fast_transformer\": use_fast_transformer,\n",
        "    \"nlayers_cls\": nlayers_cls,\n",
        "    \"n_cls\": n_cls,\n",
        "    \"vocab\": vocab,\n",
        "}\n",
        "\n",
        "sig = inspect.signature(TransformerModel.__init__)\n",
        "supported = set(sig.parameters.keys())\n",
        "model_kwargs = {k: v for k, v in candidate_kwargs.items() if k in supported}\n",
        "\n",
        "model = TransformerModel(**model_kwargs)\n",
        "\n",
        "# 4) Checkpoint laden\n",
        "ckpt_candidates = [\n",
        "    os.path.join(SCGPT_MODEL_DIR, \"model.pt\"),\n",
        "    os.path.join(SCGPT_MODEL_DIR, \"best_model.pt\"),\n",
        "]\n",
        "ckpt_path = next((p for p in ckpt_candidates if os.path.exists(p)), None)\n",
        "if ckpt_path is None:\n",
        "    raise FileNotFoundError(\n",
        "        f\"Kein Checkpoint gefunden. Erwartet: {ckpt_candidates}\"\n",
        "    )\n",
        "\n",
        "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
        "if isinstance(state, dict) and \"model_state_dict\" in state:\n",
        "    state = state[\"model_state_dict\"]\n",
        "\n",
        "missing, unexpected = model.load_state_dict(state, strict=False)\n",
        "\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "print(f\"Loaded scGPT from: {ckpt_path}\")\n",
        "print(f\"Missing keys: {len(missing)} | Unexpected keys: {len(unexpected)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7b72e09",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "# ======================\n",
        "# 4C) Batch inference -> mlm_output\n",
        "# ======================\n",
        "PAD_ID = vocab[pad_token] if \"pad_token\" in globals() and pad_token in vocab else (vocab[\"<pad>\"] if \"<pad>\" in vocab else 0)\n",
        "UNK_ID = vocab[\"<unk>\"] if \"<unk>\" in vocab else PAD_ID\n",
        "\n",
        "def genes_to_ids(gene_sym_row):\n",
        "    ids = []\n",
        "    for g in gene_sym_row:\n",
        "        g = str(g)\n",
        "        if g in vocab:\n",
        "            ids.append(vocab[g])\n",
        "        else:\n",
        "            ids.append(UNK_ID)\n",
        "    return np.array(ids, dtype=np.int64)\n",
        "\n",
        "def apply_value_mask(values_batch, mlm_prob, mask_value, seed):\n",
        "    \"\"\"\n",
        "    values_batch: np.ndarray (B, K) float32\n",
        "    maskt zufaellig mlm_prob-Anteil der Positionen.\n",
        "    Gibt masked_values und mask_matrix zurueck.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    masked = values_batch.copy()\n",
        "\n",
        "    B, K = masked.shape\n",
        "    mask_matrix = rng.random((B, K)) < mlm_prob\n",
        "\n",
        "    # Optional: falls du ein spezielles 1. Token haettest,\n",
        "    # koenntest du mask_matrix[:, 0] = False setzen.\n",
        "    masked[mask_matrix] = mask_value\n",
        "    return masked, mask_matrix\n",
        "\n",
        "def scgpt_mlm_scores_batch(gene_syms_batch, values_batch):\n",
        "    src = np.stack([genes_to_ids(row) for row in gene_syms_batch], axis=0)  # (B,K)\n",
        "    vals = values_batch.astype(np.float32)\n",
        "\n",
        "    # deterministisches masking pro batch-call\n",
        "    vals_masked, mask_matrix = apply_value_mask(\n",
        "        vals, mlm_prob=MLM_PROB, mask_value=MASK_VALUE, seed=SEED + int(torch.randint(0, 10_000, (1,)).item())\n",
        "    )\n",
        "\n",
        "    src_t = torch.from_numpy(src).to(DEVICE)\n",
        "    vals_t = torch.from_numpy(vals_masked).to(DEVICE)\n",
        "\n",
        "    pad_mask = torch.zeros(src_t.shape, dtype=torch.bool, device=DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(\n",
        "            src=src_t,\n",
        "            values=vals_t,\n",
        "            src_key_padding_mask=pad_mask,\n",
        "            output_hidden_states=False,\n",
        "            CLS=False,\n",
        "        )\n",
        "        mlm = out.get(\"mlm_output\", None)\n",
        "        if mlm is None:\n",
        "            raise KeyError(\"mlm_output nicht im scGPT output. Prüfe scGPT-Version/Config.\")\n",
        "        if mlm.ndim == 3 and mlm.shape[-1] == 1:\n",
        "            mlm = mlm.squeeze(-1)\n",
        "        mlm = mlm.detach().float().cpu().numpy()\n",
        "    return mlm  # (B,K)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafc2abd",
      "metadata": {
        "language": "python"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/931 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "TransformerModel.forward() got an unexpected keyword argument 'output_hidden_states'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m scores_acc = \u001b[32m0\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MASK_N_RUNS):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     scores_acc = scores_acc + \u001b[43mscgpt_mlm_scores_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtopk_gene_symbols\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtopk_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mend\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m scores = scores_acc / MASK_N_RUNS\n\u001b[32m     16\u001b[39m all_scores[start:end] = scores\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mscgpt_mlm_scores_batch\u001b[39m\u001b[34m(gene_syms_batch, values_batch)\u001b[39m\n\u001b[32m     46\u001b[39m pad_mask = torch.zeros(src_t.shape, dtype=torch.bool, device=DEVICE)\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m=\u001b[49m\u001b[43msrc_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvals_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCLS\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m     mlm = out.get(\u001b[33m\"\u001b[39m\u001b[33mmlm_output\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mlm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/AIFoundation/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv311/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1536\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1539\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1540\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1544\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: TransformerModel.forward() got an unexpected keyword argument 'output_hidden_states'"
          ]
        }
      ],
      "source": [
        "# ======================\n",
        "# 4D) Für alle Zellen scGPT-Scores berechnen und Sentence bauen\n",
        "# ======================\n",
        "B = 32 if DEVICE == \"cuda\" else 8\n",
        "all_scores = np.zeros((len(cell_ids), K_TOP), dtype=np.float32)\n",
        "\n",
        "for start in tqdm(range(0, len(cell_ids), B)):\n",
        "    end = min(len(cell_ids), start + B)\n",
        "    scores_acc = 0\n",
        "    for r in range(MASK_N_RUNS):\n",
        "        scores_acc = scores_acc + scgpt_mlm_scores_batch(\n",
        "            topk_gene_symbols[start:end],\n",
        "            topk_values[start:end],\n",
        "        )\n",
        "    scores = scores_acc / MASK_N_RUNS\n",
        "    all_scores[start:end] = scores\n",
        "\n",
        "scgpt_sentences = []\n",
        "for i in range(len(cell_ids)):\n",
        "    scores = all_scores[i]\n",
        "    idx = np.argsort(-scores)[:K_TOP]\n",
        "    genes_sorted = topk_gene_symbols[i, idx]\n",
        "    scgpt_sentences.append(\" \".join(map(str, genes_sorted)))\n",
        "\n",
        "df_scgpt = pd.DataFrame({\"cell_id\": cell_ids, \"sentence\": scgpt_sentences})\n",
        "scgpt_sent_path = os.path.join(PROCESSED_DIR, \"c2s_sentences_scgpt_mlm.txt\")\n",
        "with open(scgpt_sent_path, \"w\") as f:\n",
        "    for cid, sent in zip(df_scgpt[\"cell_id\"].values, df_scgpt[\"sentence\"].values):\n",
        "        f.write(f\"{cid}\\t{sent}\\n\")\n",
        "\n",
        "print(\"Wrote:\", scgpt_sent_path)\n",
        "df_scgpt.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Supervised Fine-tuning Daten bauen (Prompt → Label)\n",
        "\n",
        "Wir trainieren ein Causal LM so, dass es nach `Answer:` das Label ausgibt.\n",
        "Loss wird nur auf den Answer-Teil gerechnet (Prompt-Masking).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "PROMPT_TEMPLATE = \"Cell: {sentence}\\nTask: predict cell type.\\nAnswer:\"\n",
        "\n",
        "def build_supervised_df(df_sent: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = obs.merge(df_sent, on=\"cell_id\", how=\"inner\")\n",
        "    df[\"prompt\"] = df[\"sentence\"].apply(lambda s: PROMPT_TEMPLATE.format(sentence=s))\n",
        "    df[\"answer\"] = df[LABEL_COL].astype(str)\n",
        "    df[\"text_full\"] = df[\"prompt\"] + \" \" + df[\"answer\"]\n",
        "    return df[[\"cell_id\", \"split\", \"prompt\", \"answer\", \"text_full\"]]\n",
        "\n",
        "df_train_base = build_supervised_df(df_base)\n",
        "df_train_scgpt = build_supervised_df(df_scgpt)\n",
        "\n",
        "print(df_train_base.head(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Tokenisierung + Collator mit Prompt-Masking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "class SFTDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, tokenizer, max_len: int):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.tok = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        row = self.df.iloc[i]\n",
        "        full = row[\"text_full\"]\n",
        "        prompt = row[\"prompt\"]\n",
        "\n",
        "        enc_full = self.tok(full, truncation=True, max_length=self.max_len, padding=False)\n",
        "        enc_prompt = self.tok(prompt, truncation=True, max_length=self.max_len, padding=False)\n",
        "\n",
        "        input_ids = enc_full[\"input_ids\"]\n",
        "        attn = enc_full[\"attention_mask\"]\n",
        "\n",
        "        labels = np.array(input_ids, dtype=np.int64)\n",
        "        prompt_len = len(enc_prompt[\"input_ids\"])\n",
        "        labels[:prompt_len] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(attn, dtype=torch.long),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
        "            \"answer\": row[\"answer\"],\n",
        "        }\n",
        "\n",
        "def make_collate_fn(tokenizer):\n",
        "    def collate(batch):\n",
        "        max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
        "        input_ids = []\n",
        "        attention_mask = []\n",
        "        labels = []\n",
        "        answers = []\n",
        "        for x in batch:\n",
        "            pad = max_len - len(x[\"input_ids\"])\n",
        "            input_ids.append(torch.cat([x[\"input_ids\"], torch.full((pad,), tokenizer.pad_token_id, dtype=torch.long)]))\n",
        "            attention_mask.append(torch.cat([x[\"attention_mask\"], torch.zeros((pad,), dtype=torch.long)]))\n",
        "            labels.append(torch.cat([x[\"labels\"], torch.full((pad,), -100, dtype=torch.long)]))\n",
        "            answers.append(x[\"answer\"])\n",
        "        return {\n",
        "            \"input_ids\": torch.stack(input_ids),\n",
        "            \"attention_mask\": torch.stack(attention_mask),\n",
        "            \"labels\": torch.stack(labels),\n",
        "            \"answers\": answers,\n",
        "        }\n",
        "    return collate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) C2S Basismodell laden + LoRA anhängen\n",
        "\n",
        "Falls dein Basismodell keine `q_proj/v_proj` Module hat (z. B. GPT2), musst du `target_modules` anpassen.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(C2S_MODEL_NAME_OR_PATH, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def make_lora_model():\n",
        "    base = AutoModelForCausalLM.from_pretrained(\n",
        "        C2S_MODEL_NAME_OR_PATH,\n",
        "        torch_dtype=torch.float16 if (DEVICE == \"cuda\") else torch.float32,\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    target_modules = [\"q_proj\", \"v_proj\"]  # ggf. anpassen!\n",
        "\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=LORA_R,\n",
        "        lora_alpha=LORA_ALPHA,\n",
        "        lora_dropout=LORA_DROPOUT,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=target_modules,\n",
        "    )\n",
        "    model = get_peft_model(base, lora_cfg)\n",
        "    model.print_trainable_parameters()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Training-Funktion (gleiche Settings für beide Varianten)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "def train_one(df_all: pd.DataFrame, out_dir: str):\n",
        "    df_tr = df_all[df_all[\"split\"] == \"train\"].copy()\n",
        "    df_va = df_all[df_all[\"split\"] == \"val\"].copy()\n",
        "\n",
        "    ds_tr = SFTDataset(df_tr, tokenizer, MAX_LEN)\n",
        "    ds_va = SFTDataset(df_va, tokenizer, MAX_LEN)\n",
        "\n",
        "    model = make_lora_model()\n",
        "    collate_fn = make_collate_fn(tokenizer)\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=out_dir,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        gradient_accumulation_steps=GRAD_ACCUM,\n",
        "        num_train_epochs=EPOCHS,\n",
        "        learning_rate=LR,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=200,\n",
        "        save_steps=200,\n",
        "        save_total_limit=2,\n",
        "        logging_steps=50,\n",
        "        fp16=(DEVICE == \"cuda\"),\n",
        "        report_to=\"none\",\n",
        "        seed=SEED,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        greater_is_better=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        train_dataset=ds_tr,\n",
        "        eval_dataset=ds_va,\n",
        "        data_collator=collate_fn,\n",
        "        tokenizer=tokenizer,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(out_dir)\n",
        "    tokenizer.save_pretrained(out_dir)\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Train Baseline-LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "trainer_base = train_one(df_train_base, OUT_BASELINE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Train scGPT-LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "trainer_scgpt = train_one(df_train_scgpt, OUT_SCGPT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Evaluation: Label generieren auf Testset\n",
        "\n",
        "Wir generieren kurz nach `Answer:` und vergleichen mit Ground Truth.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def predict_labels(model_dir: str, df_all: pd.DataFrame, max_new_tokens: int = 12):\n",
        "    tok = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
        "    if tok.pad_token is None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_dir,\n",
        "        torch_dtype=torch.float16 if (DEVICE == \"cuda\") else torch.float32,\n",
        "    ).to(DEVICE)\n",
        "    model.eval()\n",
        "\n",
        "    df_te = df_all[df_all[\"split\"] == \"test\"].copy().reset_index(drop=True)\n",
        "\n",
        "    preds = []\n",
        "    trues = df_te[\"answer\"].tolist()\n",
        "\n",
        "    for i in tqdm(range(len(df_te))):\n",
        "        prompt = df_te.loc[i, \"prompt\"]\n",
        "        enc = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(DEVICE)\n",
        "        gen = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            num_beams=1,\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "        )\n",
        "        out = tok.decode(gen[0], skip_special_tokens=True)\n",
        "\n",
        "        pred = out.split(\"Answer:\", 1)[1].strip() if \"Answer:\" in out else out.strip()\n",
        "        pred = pred.split(\"\\n\")[0].strip()\n",
        "        pred = pred.split(\".\")[0].split(\",\")[0].strip()\n",
        "        preds.append(pred)\n",
        "\n",
        "    return trues, preds\n",
        "\n",
        "def evaluate(trues, preds):\n",
        "    return accuracy_score(trues, preds), f1_score(trues, preds, average=\"macro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "y_true_b, y_pred_b = predict_labels(OUT_BASELINE, df_train_base)\n",
        "acc_b, f1_b = evaluate(y_true_b, y_pred_b)\n",
        "print(\"BASELINE  acc:\", acc_b, \"macroF1:\", f1_b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "y_true_s, y_pred_s = predict_labels(OUT_SCGPT, df_train_scgpt)\n",
        "acc_s, f1_s = evaluate(y_true_s, y_pred_s)\n",
        "print(\"scGPT     acc:\", acc_s, \"macroF1:\", f1_s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting (kurz)\n",
        "\n",
        "**OOM / zu langsam**\n",
        "- `K_TOP=256`\n",
        "- `MAX_LEN` runter\n",
        "- `BATCH_SIZE=1`, `GRAD_ACCUM` hoch\n",
        "- `LORA_R=4`\n",
        "\n",
        "**LoRA target_modules passt nicht**\n",
        "- Für GPT2-artige Modelle: oft `\"c_attn\"` / `\"c_proj\"` statt `q_proj/v_proj`.\n",
        "- Einfach `print(model)` und nach Modulnamen suchen.\n",
        "\n",
        "**Labels passen nicht**\n",
        "- `LABEL_COL` manuell setzen.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2S Tutorial-Style 1: Finetuning On New Dataset\n",
    "\n",
    "Dieses Notebook ist am Workflow von:\n",
    "- `c2s_tutorial_3_finetuning_on_new_datasets.ipynb`\n",
    "\n",
    "Ziel: `vandijklab/C2S-Pythia-410m-cell-type-prediction` auf deinem Datensatz feinjustieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Optional (bei Bedarf):\n",
    "# %pip install -q cell2sentence anndata scanpy transformers datasets pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/AI-Biomedicine/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "\n",
    "import cell2sentence as cs\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN_DIR: /root/AI-Biomedicine/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/notebooks/c2s_tutorial_style/runs/2026-02-26_13-42-01\n"
     ]
    }
   ],
   "source": [
    "# ---------- Config ----------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "H5AD_PATH = Path('../../data/dominguez_conde_immune_tissue_two_donors.h5ad')\n",
    "BASE_MODEL = 'vandijklab/C2S-Pythia-410m-cell-type-prediction'\n",
    "TRAINING_TASK = 'cell_type_prediction'\n",
    "TOP_K_GENES = 200\n",
    "\n",
    "RUN_NAME = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "RUN_DIR = Path('./runs') / RUN_NAME\n",
    "CSDATA_DIR = RUN_DIR / 'csdata'\n",
    "MODEL_DIR = RUN_DIR / 'model'\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSDATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert H5AD_PATH.exists(), f'Not found: {H5AD_PATH.resolve()}'\n",
    "print('RUN_DIR:', RUN_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (29773, 36503)\n",
      "obs columns: ['cell_type', 'tissue', 'batch_condition', 'organism', 'assay', 'sex']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Load dataset ----------\n",
    "adata = ad.read_h5ad(H5AD_PATH)\n",
    "print('shape:', adata.shape)\n",
    "print('obs columns:', list(adata.obs.columns))\n",
    "\n",
    "if 'cell_type' not in adata.obs.columns:\n",
    "    raise ValueError(\"adata.obs must contain 'cell_type' for training labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label columns: ['cell_type', 'tissue', 'batch_condition', 'organism', 'sex']\n"
     ]
    }
   ],
   "source": [
    "# ---------- Minimal preprocessing (tutorial-style baseline) ----------\n",
    "adata = adata.copy()\n",
    "adata.var_names_make_unique()\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "\n",
    "label_cols = [c for c in ['cell_type', 'tissue', 'batch_condition', 'organism', 'sex'] if c in adata.obs.columns]\n",
    "print('label columns:', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN: more variables (36503) than observations (29773)... did you mean to transpose the object (e.g. adata.T)?\n",
      "WARN: more variables (36503) than observations (29773), did you mean to transpose the object (e.g. adata.T)?\n",
      "100%|██████████| 29773/29773 [00:10<00:00, 2840.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n samples in arrow: 29773\n",
      "vocab size: 36503\n"
     ]
    }
   ],
   "source": [
    "# ---------- AnnData -> Arrow + vocabulary ----------\n",
    "arrow_ds, vocab = cs.CSData.adata_to_arrow(\n",
    "    adata,\n",
    "    random_state=SEED,\n",
    "    sentence_delimiter=' ',\n",
    "    label_col_names=label_cols,\n",
    ")\n",
    "print('n samples in arrow:', len(arrow_ds))\n",
    "print('vocab size:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /root/AI-Biomedicine/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/notebooks/c2s_tutorial_style/runs/2026-02-26_13-42-01/split_indices.json\n",
      "{'train': 23847, 'val': 2948, 'test': 2978}\n"
     ]
    }
   ],
   "source": [
    "# ---------- Train/val/test split ----------\n",
    "_, split_indices = cs.utils.train_test_split_arrow_ds(arrow_ds)\n",
    "\n",
    "split_path = RUN_DIR / 'split_indices.json'\n",
    "with split_path.open('w') as f:\n",
    "    json.dump(split_indices, f, indent=2)\n",
    "\n",
    "print('saved:', split_path.resolve())\n",
    "print({k: len(v) for k, v in split_indices.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 29773/29773 [00:00<00:00, 55301.64 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSData Object; Path=runs/2026-02-26_13-42-01/csdata/dataset_arrow, Format=arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- Save CSData ----------\n",
    "csdata = cs.CSData.csdata_from_arrow(\n",
    "    arrow_dataset=arrow_ds,\n",
    "    vocabulary=vocab,\n",
    "    save_dir=str(CSDATA_DIR),\n",
    "    save_name='dataset_arrow',\n",
    "    dataset_backend='arrow',\n",
    ")\n",
    "print(csdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 292/292 [00:00<00:00, 362.74it/s, Materializing param=gpt_neox.layers.23.post_attention_layernorm.weight] \n",
      "Writing model shards: 100%|██████████| 1/1 [00:04<00:00,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base model: vandijklab/C2S-Pythia-410m-cell-type-prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------- Init model ----------\n",
    "csmodel = cs.CSModel(\n",
    "    model_name_or_path=BASE_MODEL,\n",
    "    save_dir=str(MODEL_DIR),\n",
    "    save_name='finetuned_cell_type_prediction',\n",
    ")\n",
    "print('base model:', BASE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(\n",
       "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       "adam_beta1=0.9,\n",
       "adam_beta2=0.999,\n",
       "adam_epsilon=1e-08,\n",
       "auto_find_batch_size=False,\n",
       "average_tokens_across_devices=True,\n",
       "batch_eval_metrics=False,\n",
       "bf16=True,\n",
       "bf16_full_eval=False,\n",
       "data_seed=None,\n",
       "dataloader_drop_last=False,\n",
       "dataloader_num_workers=0,\n",
       "dataloader_persistent_workers=False,\n",
       "dataloader_pin_memory=False,\n",
       "dataloader_prefetch_factor=None,\n",
       "ddp_backend=None,\n",
       "ddp_broadcast_buffers=None,\n",
       "ddp_bucket_cap_mb=None,\n",
       "ddp_find_unused_parameters=None,\n",
       "ddp_timeout=1800,\n",
       "debug=[],\n",
       "deepspeed=None,\n",
       "disable_tqdm=False,\n",
       "do_eval=True,\n",
       "do_predict=False,\n",
       "do_train=False,\n",
       "enable_jit_checkpoint=False,\n",
       "eval_accumulation_steps=None,\n",
       "eval_delay=0,\n",
       "eval_do_concat_batches=True,\n",
       "eval_on_start=False,\n",
       "eval_steps=50,\n",
       "eval_strategy=IntervalStrategy.STEPS,\n",
       "eval_use_gather_object=False,\n",
       "fp16=False,\n",
       "fp16_full_eval=False,\n",
       "fsdp=[],\n",
       "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       "full_determinism=False,\n",
       "gradient_accumulation_steps=4,\n",
       "gradient_checkpointing=False,\n",
       "gradient_checkpointing_kwargs=None,\n",
       "greater_is_better=False,\n",
       "hub_always_push=False,\n",
       "hub_model_id=None,\n",
       "hub_private_repo=None,\n",
       "hub_revision=None,\n",
       "hub_strategy=HubStrategy.EVERY_SAVE,\n",
       "hub_token=<HUB_TOKEN>,\n",
       "ignore_data_skip=False,\n",
       "include_for_metrics=[],\n",
       "include_num_input_tokens_seen=no,\n",
       "label_names=None,\n",
       "label_smoothing_factor=0.0,\n",
       "learning_rate=1e-05,\n",
       "length_column_name=length,\n",
       "liger_kernel_config=None,\n",
       "load_best_model_at_end=True,\n",
       "local_rank=-1,\n",
       "log_level=passive,\n",
       "log_level_replica=warning,\n",
       "log_on_each_node=True,\n",
       "logging_dir=None,\n",
       "logging_first_step=False,\n",
       "logging_nan_inf_filter=True,\n",
       "logging_steps=50,\n",
       "logging_strategy=IntervalStrategy.STEPS,\n",
       "lr_scheduler_kwargs=None,\n",
       "lr_scheduler_type=SchedulerType.COSINE,\n",
       "max_grad_norm=1.0,\n",
       "max_steps=-1,\n",
       "metric_for_best_model=loss,\n",
       "neftune_noise_alpha=None,\n",
       "num_train_epochs=5,\n",
       "optim=OptimizerNames.ADAMW_TORCH_FUSED,\n",
       "optim_args=None,\n",
       "optim_target_modules=None,\n",
       "output_dir=runs/2026-02-26_13-42-01/model/hf_trainer_output,\n",
       "parallelism_config=None,\n",
       "per_device_eval_batch_size=8,\n",
       "per_device_train_batch_size=8,\n",
       "prediction_loss_only=False,\n",
       "project=huggingface,\n",
       "push_to_hub=False,\n",
       "remove_unused_columns=True,\n",
       "report_to=[],\n",
       "restore_callback_states_from_checkpoint=False,\n",
       "resume_from_checkpoint=None,\n",
       "run_name=None,\n",
       "save_on_each_node=False,\n",
       "save_only_model=False,\n",
       "save_steps=100,\n",
       "save_strategy=SaveStrategy.STEPS,\n",
       "save_total_limit=2,\n",
       "seed=42,\n",
       "skip_memory_metrics=True,\n",
       "tf32=None,\n",
       "torch_compile=False,\n",
       "torch_compile_backend=None,\n",
       "torch_compile_mode=None,\n",
       "torch_empty_cache_steps=None,\n",
       "trackio_space_id=trackio,\n",
       "train_sampling_strategy=random,\n",
       "use_cache=False,\n",
       "use_cpu=True,\n",
       "use_liger_kernel=False,\n",
       "warmup_ratio=None,\n",
       "warmup_steps=0.05,\n",
       "weight_decay=0.0,\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- TrainingArguments ----------\n",
    "HF_OUTPUT_DIR = MODEL_DIR / 'hf_trainer_output'\n",
    "HF_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=str(HF_OUTPUT_DIR),\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    warmup_steps=0.05,\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none',\n",
    "    use_cpu=True\n",
    ")\n",
    "train_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model from path on disk: runs/2026-02-26_13-42-01/model/finetuned_cell_type_prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 292/292 [00:00<00:00, 401.01it/s, Materializing param=gpt_neox.layers.23.post_attention_layernorm.weight] \n",
      "Map (num_proc=3): 100%|██████████| 29773/29773 [00:17<00:00, 1694.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training. Output directory: runs/2026-02-26_13-42-01/model/hf_trainer_output\n",
      "Selecting 500 samples of eval dataset to shorten validation loop.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Trainer.__init__() got an unexpected keyword argument 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ---------- Fine-tune ----------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mcsmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfine_tune\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcsdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcsdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAINING_TASK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_on_response_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k_genes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K_GENES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_eval_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_split_indices_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_indices\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mFine-tuning finished.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI-Biomedicine/Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings/.venv/lib/python3.13/site-packages/cell2sentence/csmodel.py:210\u001b[39m, in \u001b[36mCSModel.fine_tune\u001b[39m\u001b[34m(self, csdata, task, train_args, loss_on_response_only, top_k_genes, max_eval_samples, data_split_indices_dict, prompt_formatter, formatted_hf_ds, num_proc)\u001b[39m\n\u001b[32m    207\u001b[39m     eval_dataset = eval_dataset.select(sampled_eval_indices)\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# Define Trainer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m trainer = \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m trainer.train()\n\u001b[32m    219\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFinetuning completed. Updated model saved to disk at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Trainer.__init__() got an unexpected keyword argument 'tokenizer'"
     ]
    }
   ],
   "source": [
    "# ---------- Fine-tune ----------\n",
    "csmodel.fine_tune(\n",
    "    csdata=csdata,\n",
    "    task=TRAINING_TASK,\n",
    "    train_args=train_args,\n",
    "    loss_on_response_only=False,\n",
    "    top_k_genes=TOP_K_GENES,\n",
    "    max_eval_samples=500,\n",
    "    data_split_indices_dict={\n",
    "        'train': split_indices['train'],\n",
    "        'val': split_indices['val'],\n",
    "        'test': split_indices.get('test', []),\n",
    "    },\n",
    ")\n",
    "print('Fine-tuning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save run metadata for Notebook 2 ----------\n",
    "run_info = {\n",
    "    'h5ad_path': str(H5AD_PATH),\n",
    "    'base_model': BASE_MODEL,\n",
    "    'training_task': TRAINING_TASK,\n",
    "    'top_k_genes': TOP_K_GENES,\n",
    "    'run_dir': str(RUN_DIR.resolve()),\n",
    "    'csdata_dir': str(CSDATA_DIR.resolve()),\n",
    "    'model_dir': str(MODEL_DIR.resolve()),\n",
    "    'finetuned_model_path': str((MODEL_DIR / 'finetuned_cell_type_prediction').resolve()),\n",
    "    'split_indices_path': str(split_path.resolve()),\n",
    "}\n",
    "\n",
    "run_info_path = RUN_DIR / 'run_info.json'\n",
    "with run_info_path.open('w') as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print('saved:', run_info_path.resolve())\n",
    "run_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

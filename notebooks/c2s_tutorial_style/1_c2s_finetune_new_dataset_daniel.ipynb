{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C2S Tutorial-Style 1: Finetuning On New Dataset\n",
    "\n",
    "Dieses Notebook ist am Workflow von:\n",
    "- `c2s_tutorial_3_finetuning_on_new_datasets.ipynb`\n",
    "\n",
    "Ziel: `vandijklab/C2S-Pythia-410m-cell-type-prediction` auf deinem Datensatz feinjustieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24ff4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional (bei Bedarf):\n",
    "# %pip install -q cell2sentence anndata scanpy transformers datasets pandas numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a636ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python built-in libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import random\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    import torch\n",
    "except RuntimeError as e:\n",
    "    _msg = str(e)\n",
    "    if ('RpcBackendOptions' in _msg) or ('already has a docstring' in _msg):\n",
    "        raise RuntimeError(\n",
    "            'Torch is in a partially initialized state in this kernel. '\n",
    "            'Restart the Jupyter kernel, then run the notebook from the first cell.'\n",
    "        ) from e\n",
    "    raise\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from transformers.utils import logging as hf_logging\n",
    "hf_logging.enable_progress_bar()\n",
    "\n",
    "# Single-cell libraries\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "\n",
    "# Cell2Sentence imports\n",
    "import cell2sentence as cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4bdd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Config ----------\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = Path('../../data/dominguez_conde_immune_tissue_two_donors.h5ad')\n",
    "BASE_MODEL = 'vandijklab/C2S-Pythia-410m-cell-type-prediction'\n",
    "TRAINING_TASK = 'cell_type_prediction'\n",
    "TOP_K_GENES = 128\n",
    "\n",
    "RUN_NAME = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "RUN_DIR = Path('./runs') / RUN_NAME\n",
    "CSDATA_DIR = RUN_DIR / 'csdata'\n",
    "MODEL_DIR = RUN_DIR / 'model'\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CSDATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert DATA_PATH.exists(), f'Not found: {DATA_PATH.resolve()}'\n",
    "print('RUN_DIR:', RUN_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e800fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Load dataset ----------\n",
    "adata = anndata.read_h5ad(DATA_PATH)\n",
    "print('shape:', adata.shape)\n",
    "print('obs columns:', list(adata.obs.columns))\n",
    "\n",
    "if 'cell_type' not in adata.obs.columns:\n",
    "    raise ValueError(\"adata.obs must contain 'cell_type' for training labels.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce6249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Minimal preprocessing (tutorial-style baseline) ----------\n",
    "adata = adata.copy()\n",
    "# adata.var_names_make_unique()\n",
    "sc.pp.filter_cells(adata, min_genes=200)\n",
    "sc.pp.filter_genes(adata, min_cells=3)\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata, base=10)\n",
    "\n",
    "# label_cols = [c for c in ['cell_type', 'tissue', 'batch_condition', 'organism', 'sex'] if c in adata.obs.columns]\n",
    "\n",
    "adata_obs_cols_to_keep = [\"cell_type\", \"tissue\", \"batch_condition\", \"organism\", \"sex\"]\n",
    "# print('label columns:', label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8893ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- AnnData -> Arrow + vocabulary ----------\n",
    "arrow_ds, vocab = cs.CSData.adata_to_arrow(\n",
    "    adata,\n",
    "    random_state=SEED,\n",
    "    sentence_delimiter=' ',\n",
    "    label_col_names=adata_obs_cols_to_keep,\n",
    ")\n",
    "print('n samples in arrow:', len(arrow_ds))\n",
    "print('vocab size:', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746b6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------- Train/val/test split ----------\n",
    "# _, split_indices = cs.utils.train_test_split_arrow_ds(arrow_ds)\n",
    "\n",
    "# split_path = RUN_DIR / 'split_indices.json'\n",
    "# with split_path.open('w') as f:\n",
    "#     json.dump(split_indices, f, indent=2)\n",
    "\n",
    "# print('saved:', split_path.resolve())\n",
    "# print({k: len(v) for k, v in split_indices.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save CSData ----------\n",
    "csdata = cs.CSData.csdata_from_arrow(\n",
    "    arrow_dataset=arrow_ds,\n",
    "    vocabulary=vocab,\n",
    "    save_dir=str(CSDATA_DIR),\n",
    "    save_name='dataset_arrow',\n",
    "    dataset_backend='arrow',\n",
    ")\n",
    "print(csdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04726f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Init model ----------\n",
    "csmodel = cs.CSModel(\n",
    "    model_name_or_path=BASE_MODEL,\n",
    "    save_dir=str(MODEL_DIR),\n",
    "    save_name='finetuned_cell_type_prediction',\n",
    ")\n",
    "print('model:', csmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b896f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- TrainingArguments ----------\n",
    "HF_OUTPUT_DIR = MODEL_DIR / 'hf_trainer_output'\n",
    "HF_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=str(HF_OUTPUT_DIR),\n",
    "    bf16=torch.cuda.is_available() and torch.cuda.is_bf16_supported(),\n",
    "    fp16=torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()),\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    warmup_steps=1, # warupsteps are used to stabilize training in early iterations, especially when using a low learning rate\n",
    "    lr_scheduler_type='cosine',\n",
    "    logging_steps=1,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    save_strategy='steps',\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to='none'\n",
    ")\n",
    "train_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Fine-tune ----------\n",
    "# Compatibility shim for newer transformers versions where Trainer uses\n",
    "# `processing_class` instead of `tokenizer`.\n",
    "import inspect\n",
    "from transformers import Trainer\n",
    "\n",
    "_trainer_sig = inspect.signature(Trainer.__init__).parameters\n",
    "if ('processing_class' in _trainer_sig) and ('tokenizer' not in _trainer_sig):\n",
    "    _orig_trainer_init = Trainer.__init__\n",
    "    def _trainer_init_compat(self, *args, **kwargs):\n",
    "        if 'tokenizer' in kwargs and 'processing_class' not in kwargs:\n",
    "            kwargs['processing_class'] = kwargs.pop('tokenizer')\n",
    "        return _orig_trainer_init(self, *args, **kwargs)\n",
    "    Trainer.__init__ = _trainer_init_compat\n",
    "\n",
    "# Prefer periodic text logs over tqdm progress bars\n",
    "if hasattr(train_args, 'disable_tqdm'):\n",
    "    train_args.disable_tqdm = True\n",
    "if hasattr(train_args, 'logging_strategy'):\n",
    "    train_args.logging_strategy = 'steps'\n",
    "if hasattr(train_args, 'logging_steps'):\n",
    "    train_args.logging_steps = max(1, int(getattr(train_args, 'logging_steps', 50) or 50))\n",
    "if hasattr(train_args, 'report_to'):\n",
    "    train_args.report_to = []\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(f\"Training heartbeat every {getattr(train_args, 'logging_steps', 'N/A')} steps.\")\n",
    "\n",
    "csmodel.fine_tune(\n",
    "    csdata=csdata,\n",
    "    task=TRAINING_TASK,\n",
    "    train_args=train_args,\n",
    "    loss_on_response_only=False,\n",
    "    top_k_genes=TOP_K_GENES,\n",
    "    max_eval_samples=10, \n",
    "    num_proc=1,\n",
    "    # data_split_indices_dict={\n",
    "    #     'train': split_indices['train'],\n",
    "    #     'val': split_indices['val'],\n",
    "    #     'test': split_indices.get('test', []),\n",
    "    # },\n",
    ")\n",
    "print('Fine-tuning finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Save run metadata for Notebook 2 ----------\n",
    "run_info = {\n",
    "    'h5ad_path': str(DATA_PATH),\n",
    "    'base_model': BASE_MODEL,\n",
    "    'training_task': TRAINING_TASK,\n",
    "    'top_k_genes': TOP_K_GENES,\n",
    "    'run_dir': str(RUN_DIR.resolve()),\n",
    "    'csdata_dir': str(CSDATA_DIR.resolve()),\n",
    "    'model_dir': str(MODEL_DIR.resolve()),\n",
    "    'finetuned_model_path': str((MODEL_DIR / 'finetuned_cell_type_prediction').resolve()),\n",
    "    # 'split_indices_path': str(split_path.resolve()),\n",
    "}\n",
    "\n",
    "run_info_path = RUN_DIR / 'run_info.json'\n",
    "with run_info_path.open('w') as f:\n",
    "    json.dump(run_info, f, indent=2)\n",
    "\n",
    "print('saved:', run_info_path.resolve())\n",
    "run_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

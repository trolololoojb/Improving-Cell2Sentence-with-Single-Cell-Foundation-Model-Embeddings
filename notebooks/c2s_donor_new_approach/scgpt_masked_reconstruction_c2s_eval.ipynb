{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# scGPT Gene Masking → Reconstruction → C2S Cell-Type Evaluation\n",
    "\n",
    "**Pipeline:**\n",
    "1. Load 24 donor cells (same sample as `c2s_donor_celltype_prediction.ipynb`)\n",
    "2. **Mask** – randomly zero out `MASK_FRACTION` of each cell's expressed genes\n",
    "3. **Reconstruct** – feed masked cells through `tdc/scGPT` (with `-1` mask tokens)\n",
    "   and replace masked positions with scGPT's predicted expression values\n",
    "4. **Evaluate** – run C2S cell-type prediction on three AnnData objects:\n",
    "   - `original` (clean baseline)\n",
    "   - `masked`   (corrupted, masked genes → 0)\n",
    "   - `reconstructed` (scGPT-repaired)\n",
    "5. **Compare** results side-by-side using the 3-tier accuracy metric\n",
    "   (Exactly correct / Partly correct / Not correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# No extra installs needed – everything required is already in the environment.\n",
    "# (torch, numpy, scanpy, anndata, cell2sentence, transformers, tqdm)\n",
    "print('Environment ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-imports2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import torch\n",
    "\n",
    "import cell2sentence as cs\n",
    "from cell2sentence.tasks import predict_cell_types_of_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ── Configuration ─────────────────────────────────────────────────────────────\n",
    "SEED          = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "H5AD_PATH     = Path('../../data/dominguez_conde_immune_tissue_two_donors.h5ad')\n",
    "DONOR_COLUMN  = 'batch_condition'\n",
    "DONOR_VALUE   = 'A29'\n",
    "N_CELLS       = 24\n",
    "TOP_K_GENES   = 200          # genes passed to C2S\n",
    "MASK_FRACTION = 0.40         # fraction of expressed genes to mask per cell\n",
    "# MASK_VALUE and PAD_VALUE come from the model's args.json (loaded later):\n",
    "#   mask_value = -1  (sentinel for positions to reconstruct)\n",
    "#   pad_value  = -2  (padding, distinct from mask)\n",
    "C2S_MODEL     = 'vandijklab/C2S-Pythia-410m-cell-type-prediction'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "assert H5AD_PATH.exists(), f'File not found: {H5AD_PATH.resolve()}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-section1",
   "metadata": {},
   "source": [
    "## 1 · Load data & sample 24 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (29773, 36503)\n",
      "Donor A29: 17327 cells  →  sampled 24\n",
      "cell_type\n",
      "macrophage                                               4\n",
      "memory B cell                                            3\n",
      "naive thymus-derived CD4-positive, alpha-beta T cell     3\n",
      "T follicular helper cell                                 2\n",
      "classical monocyte                                       2\n",
      "plasma cell                                              2\n",
      "CD4-positive helper T cell                               1\n",
      "CD16-negative, CD56-bright natural killer cell, human    1\n",
      "alveolar macrophage                                      1\n",
      "effector memory CD4-positive, alpha-beta T cell          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "adata = ad.read_h5ad(H5AD_PATH)\n",
    "print('Full dataset shape:', adata.shape)\n",
    "\n",
    "adata_donor = adata[adata.obs[DONOR_COLUMN] == DONOR_VALUE].copy()\n",
    "rng = np.random.default_rng(SEED)\n",
    "idx = rng.choice(adata_donor.n_obs, size=N_CELLS, replace=False)\n",
    "adata_small = adata_donor[idx].copy()\n",
    "\n",
    "print(f'Donor {DONOR_VALUE}: {adata_donor.n_obs} cells  →  sampled {adata_small.n_obs}')\n",
    "print(adata_small.obs['cell_type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-section2",
   "metadata": {},
   "source": [
    "## 2 · Preprocessing (normalize + log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-preprocess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression matrix: (24, 36503)\n",
      "Sparsity: 95.3% zeros per cell on average\n"
     ]
    }
   ],
   "source": [
    "adata_small.var_names_make_unique()\n",
    "sc.pp.normalize_total(adata_small, target_sum=1e4)\n",
    "sc.pp.log1p(adata_small)\n",
    "\n",
    "# Dense expression matrix  (n_cells × n_genes)\n",
    "import scipy.sparse as sp\n",
    "X_orig = adata_small.X.toarray() if sp.issparse(adata_small.X) else adata_small.X.copy()\n",
    "gene_names = np.array(adata_small.var_names.tolist())\n",
    "\n",
    "print('Expression matrix:', X_orig.shape)\n",
    "print(f'Sparsity: {(X_orig == 0).mean()*100:.1f}% zeros per cell on average')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-section3",
   "metadata": {},
   "source": [
    "## 3 · Random gene masking\n",
    "\n",
    "For each cell, `MASK_FRACTION` of the expressed (non-zero) genes are set to  \n",
    "`MASK_VALUE = -1` internally. A separate zero-copy is created for C2S  \n",
    "(since C2S expects non-negative expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-mask",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked 16337 genes across 24 cells (~681 per cell, 40% of expressed)\n"
     ]
    }
   ],
   "source": [
    "# X_scgpt_in : expression matrix sent to scGPT  (masked genes = -1, kept in tokenizer)\n",
    "# X_masked   : expression matrix for C2S masked baseline  (masked genes = 0)\n",
    "# mask_record: list of boolean arrays tracking which gene positions were masked\n",
    "\n",
    "rng_mask = np.random.default_rng(SEED)\n",
    "\n",
    "X_scgpt_in = X_orig.copy()\n",
    "X_masked   = X_orig.copy()\n",
    "mask_record = []          # mask_record[i] -> indices of masked genes in cell i\n",
    "\n",
    "total_masked = 0\n",
    "for i in range(X_orig.shape[0]):\n",
    "    expressed_idx = np.where(X_orig[i] > 0)[0]\n",
    "    n_mask = max(1, int(len(expressed_idx) * MASK_FRACTION))\n",
    "    masked_idx = rng_mask.choice(expressed_idx, size=n_mask, replace=False)\n",
    "\n",
    "    X_scgpt_in[i, masked_idx] = MASK_VALUE  # -1  → tokenizer keeps these\n",
    "    X_masked[i, masked_idx]   = 0.0         # 0   → removed from C2S\n",
    "\n",
    "    mask_record.append(masked_idx)\n",
    "    total_masked += n_mask\n",
    "\n",
    "print(f'Masked {total_masked} genes across {N_CELLS} cells'\n",
    "      f' (~{total_masked/N_CELLS:.0f} per cell, {MASK_FRACTION*100:.0f}% of expressed)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-section4",
   "metadata": {},
   "source": [
    "\n",
    "## 4 · scGPT reconstruction  *(no TDC – native scgpt package)*\n",
    "\n",
    "We load the model via `huggingface_hub.snapshot_download` and the native\n",
    "`scgpt.model.TransformerModel` + `scgpt.tokenizer.GeneVocab`.\n",
    "\n",
    "**Masking strategy:** masked genes are set to `-1` (scGPT's standard mask sentinel).\n",
    "Since `-1 ≠ 0`, they stay in the gene sequence passed to the model.  \n",
    "`mlm_output` predicts expression at **every position** – including the `-1` ones.  \n",
    "Those predicted values replace the masked genes in `X_reconstructed`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-scgpt-load",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel + GeneVocab defined (no scgpt library used).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ── scGPT model – self-contained PyTorch implementation ───────────────────────\n",
    "# Adapted from https://github.com/bowang-lab/scGPT (MIT License).\n",
    "# No scgpt library required – only torch + huggingface_hub.\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Mapping, Optional, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "\n",
    "# ── GeneVocab: replaces scgpt.tokenizer.GeneVocab ────────────────────────────\n",
    "class GeneVocab:\n",
    "    \"\"\"Minimal gene-name ↔ token-ID vocabulary loaded from vocab.json.\"\"\"\n",
    "\n",
    "    def __init__(self, gene_to_id: dict):\n",
    "        self._g2i = gene_to_id\n",
    "        self._i2g = {v: k for k, v in gene_to_id.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, path):\n",
    "        with open(path) as f:\n",
    "            data = json.load(f)\n",
    "        # Format A: {\"gene_name\": int_id, ...}\n",
    "        if isinstance(data, dict) and all(isinstance(v, int) for v in data.values()):\n",
    "            return cls(data)\n",
    "        # Format B: torchtext-style {\"itos\": [...], ...}\n",
    "        if isinstance(data, dict) and \"itos\" in data:\n",
    "            return cls({t: i for i, t in enumerate(data[\"itos\"])})\n",
    "        # Format C: list of tokens\n",
    "        if isinstance(data, list):\n",
    "            return cls({t: i for i, t in enumerate(data)})\n",
    "        raise ValueError(f\"Cannot parse vocab format in {path}\")\n",
    "\n",
    "    def __contains__(self, gene):  return gene in self._g2i\n",
    "    def __getitem__(self, gene):   return self._g2i[gene]\n",
    "    def __len__(self):             return len(self._g2i)\n",
    "\n",
    "\n",
    "# ── Sub-modules ───────────────────────────────────────────────────────────────\n",
    "class GeneEncoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "        self.enc_norm  = nn.LayerNorm(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        return self.enc_norm(self.embedding(x))\n",
    "\n",
    "\n",
    "class ContinuousValueEncoder(nn.Module):\n",
    "    \"\"\"Projects scalar expression values → d_model embedding.\"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_value=512):\n",
    "        super().__init__()\n",
    "        self.max_value = max_value\n",
    "        self.linear1   = nn.Linear(1, d_model)\n",
    "        self.linear2   = nn.Linear(d_model, d_model)\n",
    "        self.norm      = nn.LayerNorm(d_model)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "    def forward(self, x):                          # x: (B, L)\n",
    "        x = x.unsqueeze(-1).clamp(-self.max_value, self.max_value)\n",
    "        x = self.linear1(x).relu()\n",
    "        return self.dropout(self.norm(self.linear2(x)))\n",
    "\n",
    "\n",
    "class CategoryValueEncoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "        self.enc_norm  = nn.LayerNorm(embedding_dim)\n",
    "    def forward(self, x):\n",
    "        return self.enc_norm(self.embedding(x.long()))\n",
    "\n",
    "\n",
    "class ExprDecoder(nn.Module):\n",
    "    \"\"\"MLP that predicts scalar expression from transformer hidden states.\"\"\"\n",
    "    def __init__(self, d_model, explicit_zero_prob=False, use_batch_labels=False):\n",
    "        super().__init__()\n",
    "        d_in = d_model * 2 if use_batch_labels else d_model\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_in, d_model), nn.LeakyReLU(),\n",
    "            nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "        self.explicit_zero_prob = explicit_zero_prob\n",
    "        if explicit_zero_prob:\n",
    "            self.zero_logit = nn.Sequential(\n",
    "                nn.Linear(d_in, d_model), nn.LeakyReLU(),\n",
    "                nn.Linear(d_model, d_model), nn.LeakyReLU(),\n",
    "                nn.Linear(d_model, 1),\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        pred = self.fc(x).squeeze(-1)           # (B, L)\n",
    "        out  = {\"pred\": pred}\n",
    "        if self.explicit_zero_prob:\n",
    "            out[\"zero_probs\"] = torch.sigmoid(self.zero_logit(x).squeeze(-1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class ClsDecoder(nn.Module):\n",
    "    def __init__(self, d_model, n_cls, nlayers=3, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers - 1):\n",
    "            layers += [nn.Linear(d_model, d_model), activation(), nn.LayerNorm(d_model)]\n",
    "        layers.append(nn.Linear(d_model, n_cls))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class MVCDecoder(nn.Module):\n",
    "    \"\"\"Masked-value-consistency decoder (inner-product style).\"\"\"\n",
    "    def __init__(self, d_model, arch_style=\"inner product\", query_activation=nn.Sigmoid,\n",
    "                 hidden_activation=nn.PReLU, explicit_zero_prob=False, use_batch_labels=False):\n",
    "        super().__init__()\n",
    "        self.arch_style        = arch_style\n",
    "        self.explicit_zero_prob = explicit_zero_prob\n",
    "        self.gene2query        = nn.Linear(d_model, d_model)\n",
    "        self.query_activation  = query_activation()\n",
    "        self.W                 = nn.Linear(d_model, d_model, bias=False)\n",
    "        if explicit_zero_prob:\n",
    "            self.fc_zero = nn.Linear(d_model, 1)\n",
    "    def forward(self, cell_emb, gene_embs):\n",
    "        query     = self.query_activation(self.gene2query(gene_embs))  # (B, L, d)\n",
    "        cell_emb_ = self.W(cell_emb).unsqueeze(2)                      # (B, d, 1)\n",
    "        pred      = torch.bmm(query, cell_emb_).squeeze(2)             # (B, L)\n",
    "        out = {\"mvc_output\": pred}\n",
    "        if self.explicit_zero_prob:\n",
    "            out[\"mvc_zero_probs\"] = torch.sigmoid(self.fc_zero(gene_embs).squeeze(-1))\n",
    "        return out\n",
    "\n",
    "\n",
    "class AdversarialDiscriminator(nn.Module):\n",
    "    def __init__(self, d_model, n_cls, nlayers=3, activation=nn.LeakyReLU, reverse_grad=False):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for _ in range(nlayers - 1):\n",
    "            layers += [nn.Linear(d_model, d_model), activation(), nn.LayerNorm(d_model)]\n",
    "        layers.append(nn.Linear(d_model, n_cls))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# ── TransformerModel ──────────────────────────────────────────────────────────\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    scGPT TransformerModel – self-contained PyTorch reimplementation.\n",
    "    Weights are fully compatible with the original scgpt library checkpoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ntoken: int, d_model: int, nhead: int, d_hid: int, nlayers: int,\n",
    "        nlayers_cls: int = 3, n_cls: int = 1, vocab=None,\n",
    "        dropout: float = 0.5, pad_token: str = \"<pad>\", pad_value: int = 0,\n",
    "        do_mvc: bool = False, do_dab: bool = False,\n",
    "        use_batch_labels: bool = False, num_batch_labels: Optional[int] = None,\n",
    "        domain_spec_batchnorm: bool = False,\n",
    "        input_emb_style: str = \"continuous\", n_input_bins: Optional[int] = None,\n",
    "        cell_emb_style: str = \"cls\", mvc_decoder_style: str = \"inner product\",\n",
    "        ecs_threshold: float = 0.3, explicit_zero_prob: bool = False,\n",
    "        use_fast_transformer: bool = False,   # ignored – always use standard attn\n",
    "        fast_transformer_backend: str = \"flash\",\n",
    "        pre_norm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model             = d_model\n",
    "        self.do_mvc              = do_mvc\n",
    "        self.do_dab              = do_dab\n",
    "        self.use_batch_labels    = use_batch_labels\n",
    "        self.cell_emb_style      = cell_emb_style\n",
    "        self.explicit_zero_prob  = explicit_zero_prob\n",
    "        self.pad_value           = pad_value\n",
    "        self.ecs_threshold       = ecs_threshold\n",
    "\n",
    "        pad_idx = vocab[pad_token] if (vocab is not None and pad_token in vocab) else pad_value\n",
    "\n",
    "        # Embeddings\n",
    "        self.encoder       = GeneEncoder(ntoken, d_model, padding_idx=pad_idx)\n",
    "        if input_emb_style == \"continuous\":\n",
    "            self.value_encoder = ContinuousValueEncoder(d_model, dropout)\n",
    "        elif input_emb_style == \"category\":\n",
    "            self.value_encoder = CategoryValueEncoder(n_input_bins, d_model, padding_idx=0)\n",
    "        else:\n",
    "            self.value_encoder = nn.Identity()\n",
    "\n",
    "        if use_batch_labels:\n",
    "            self.batch_encoder = nn.Embedding(num_batch_labels, d_model)\n",
    "\n",
    "        # Batch-norm (domain_spec_batchnorm not needed; stub keeps weight keys intact)\n",
    "        self.bn = nn.BatchNorm1d(d_model)\n",
    "        if domain_spec_batchnorm:\n",
    "            self.dsbn = nn.BatchNorm1d(d_model)   # stub\n",
    "\n",
    "        # Transformer\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=d_hid,\n",
    "            dropout=dropout, batch_first=True, norm_first=pre_norm,\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(enc_layer, num_layers=nlayers)\n",
    "\n",
    "        # Decoders\n",
    "        self.decoder     = ExprDecoder(d_model, explicit_zero_prob, use_batch_labels)\n",
    "        self.cls_decoder = ClsDecoder(d_model, n_cls, nlayers=nlayers_cls)\n",
    "        if do_mvc:\n",
    "            self.mvc_decoder = MVCDecoder(d_model, mvc_decoder_style, explicit_zero_prob=explicit_zero_prob,\n",
    "                                          use_batch_labels=use_batch_labels)\n",
    "        if do_dab:\n",
    "            self.grad_reverse_discriminator = AdversarialDiscriminator(d_model, n_cls=num_batch_labels,\n",
    "                                                                        reverse_grad=True)\n",
    "        self.sim = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "        nn.init.uniform_(self.encoder.embedding.weight, -0.1, 0.1)\n",
    "\n",
    "    def _encode(self, src, values, src_key_padding_mask, batch_labels=None):\n",
    "        src_emb = self.encoder(src)           # (B, L, d)\n",
    "        val_emb = self.value_encoder(values)  # (B, L, d)\n",
    "        emb     = src_emb + val_emb\n",
    "\n",
    "        if self.use_batch_labels and batch_labels is not None:\n",
    "            emb = emb + self.batch_encoder(batch_labels).unsqueeze(1)\n",
    "\n",
    "        B, L, d = emb.shape\n",
    "        emb = self.bn(emb.view(B * L, d)).view(B, L, d)\n",
    "\n",
    "        return self.transformer_encoder(emb, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "    def _cell_emb(self, layer_out, values=None):\n",
    "        if self.cell_emb_style == \"cls\":\n",
    "            return layer_out[:, 0, :]\n",
    "        non_pad = (values != self.pad_value).float().unsqueeze(-1) if values is not None \\\n",
    "                  else torch.ones(*layer_out.shape[:2], 1, device=layer_out.device)\n",
    "        return (layer_out * non_pad).sum(1) / non_pad.sum(1).clamp(min=1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src: Tensor, values: Tensor, src_key_padding_mask: Tensor,\n",
    "        batch_labels: Optional[Tensor] = None,\n",
    "        CLS: bool = False, CCE: bool = False, MVC: bool = False,\n",
    "        ECS: bool = False, do_sample: bool = False,\n",
    "    ) -> Mapping[str, Tensor]:\n",
    "        h      = self._encode(src, values, src_key_padding_mask, batch_labels)\n",
    "        output = {}\n",
    "\n",
    "        mlm = self.decoder(h)\n",
    "        output[\"mlm_output\"] = mlm[\"pred\"]               # (B, L)  ← used for reconstruction\n",
    "        if self.explicit_zero_prob and \"zero_probs\" in mlm:\n",
    "            output[\"mlm_zero_probs\"] = mlm[\"zero_probs\"]\n",
    "\n",
    "        cell_emb           = self._cell_emb(h, values)\n",
    "        output[\"cell_emb\"] = cell_emb\n",
    "\n",
    "        if CLS:\n",
    "            output[\"cls_output\"] = self.cls_decoder(cell_emb)\n",
    "        if MVC and self.do_mvc:\n",
    "            output.update(self.mvc_decoder(cell_emb, h))\n",
    "        if self.do_dab:\n",
    "            output[\"dab_output\"] = self.grad_reverse_discriminator(cell_emb)\n",
    "        return output\n",
    "\n",
    "\n",
    "print('TransformerModel + GeneVocab defined (no scgpt library used).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2mliwd1j9m8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using local checkpoint: C:\\Users\\Daniel\\Desktop\\GitProjects\\Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings\\models\\scGPT\n",
      "Files: ['args.json', 'best_model.pt', 'vocab.json']\n",
      "Vocab size: 60697  PAD_ID=60694  UNK_ID=60694\n",
      "pad_value=-2  mask_value=-1\n",
      "Weights loaded — missing: 38, unexpected: 25\n",
      "  Missing : ['bn.weight', 'bn.bias', 'bn.running_mean', 'bn.running_var', 'transformer_encoder.layers.0.self_attn.in_proj_weight']\n",
      "\n",
      "scGPT ready on cpu.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ── Load model from local checkpoint ─────────────────────────────────────────\n",
    "SCGPT_DIR = Path('../../models/scGPT')\n",
    "assert SCGPT_DIR.exists(), f'Model folder not found: {SCGPT_DIR.resolve()}'\n",
    "print('Using local checkpoint:', SCGPT_DIR.resolve())\n",
    "print('Files:', sorted(f.name for f in SCGPT_DIR.iterdir()))\n",
    "\n",
    "# ── Vocab ─────────────────────────────────────────────────────────────────────\n",
    "vocab = GeneVocab.from_file(str(SCGPT_DIR / 'vocab.json'))\n",
    "PAD_ID = vocab['<pad>'] if '<pad>' in vocab else 0\n",
    "UNK_ID = vocab['<unk>'] if '<unk>' in vocab else PAD_ID\n",
    "print(f'Vocab size: {len(vocab)}  PAD_ID={PAD_ID}  UNK_ID={UNK_ID}')\n",
    "\n",
    "# ── Config (args.json) ────────────────────────────────────────────────────────\n",
    "with open(SCGPT_DIR / 'args.json') as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "# Key values read from args.json:\n",
    "#   embsize=512, nheads=8, d_hid=512, nlayers=12, n_layers_cls=3\n",
    "#   input_emb_style=\"continuous\", pad_value=-2, mask_value=-1, MVC=True\n",
    "PAD_VALUE  = cfg.get('pad_value',  -2)   # -2 in this checkpoint\n",
    "MASK_VALUE = cfg.get('mask_value', -1)   # -1 (confirms our masking strategy)\n",
    "print(f'pad_value={PAD_VALUE}  mask_value={MASK_VALUE}')\n",
    "\n",
    "# ── Build architecture ────────────────────────────────────────────────────────\n",
    "scgpt_model = TransformerModel(\n",
    "    ntoken           = len(vocab),\n",
    "    d_model          = cfg['embsize'],\n",
    "    nhead            = cfg['nheads'],\n",
    "    d_hid            = cfg['d_hid'],\n",
    "    nlayers          = cfg['nlayers'],\n",
    "    nlayers_cls      = cfg.get('n_layers_cls', cfg.get('nlayers_cls', 3)),\n",
    "    n_cls            = 1,\n",
    "    vocab            = vocab,\n",
    "    dropout          = 0.0,                  # no dropout at inference\n",
    "    pad_token        = cfg.get('pad_token', '<pad>'),\n",
    "    pad_value        = PAD_VALUE,\n",
    "    do_mvc           = cfg.get('MVC', False),\n",
    "    do_dab           = cfg.get('do_dab', False),\n",
    "    use_batch_labels = cfg.get('use_batch_labels', False),\n",
    "    num_batch_labels = cfg.get('num_batch_labels', 1),\n",
    "    domain_spec_batchnorm = cfg.get('dsbn', False),\n",
    "    input_emb_style  = cfg.get('input_emb_style', 'continuous'),\n",
    "    n_input_bins     = cfg.get('n_bins', 51),\n",
    "    cell_emb_style   = 'cls' if not cfg.get('no_cls', True) else 'avg-non-pad',\n",
    "    explicit_zero_prob = cfg.get('explicit_zero_prob', False),\n",
    "    use_fast_transformer = False,           # disable flash-attn (CPU / Windows)\n",
    "    pre_norm         = cfg.get('pre_norm', False),\n",
    ")\n",
    "\n",
    "# ── Load weights ──────────────────────────────────────────────────────────────\n",
    "ckpt_path = SCGPT_DIR / 'best_model.pt'\n",
    "state = torch.load(str(ckpt_path), map_location='cpu', weights_only=False)\n",
    "if isinstance(state, dict) and 'model_state_dict' in state:\n",
    "    state = state['model_state_dict']\n",
    "\n",
    "missing, unexpected = scgpt_model.load_state_dict(state, strict=False)\n",
    "print(f'Weights loaded — missing: {len(missing)}, unexpected: {len(unexpected)}')\n",
    "if missing:\n",
    "    print('  Missing :', missing[:5])\n",
    "\n",
    "scgpt_model = scgpt_model.to(DEVICE).eval()\n",
    "print(f'\\nscGPT ready on {DEVICE}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-scgpt-reconstruct",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scGPT reconstruction:   0%|          | 0/24 [00:00<?, ?it/s]c:\\Users\\Daniel\\anaconda3\\envs\\c2s_wsl\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "scGPT reconstruction: 100%|██████████| 24/24 [00:53<00:00,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconstruction complete.\n",
      "  Avg genes in scGPT vocab / cell : 1669\n",
      "  Masked genes recovered          : 16337 / 16337  (100.0 %)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def gene_names_to_ids(names):\n",
    "    \"\"\"Map gene name strings → scGPT vocabulary IDs (UNK_ID for out-of-vocab genes).\"\"\"\n",
    "    return np.array([vocab[g] if g in vocab else UNK_ID for g in names], dtype=np.int64)\n",
    "\n",
    "# X_reconstructed: copy of original; masked gene positions will be overwritten\n",
    "X_reconstructed = X_orig.copy()\n",
    "n_in_vocab  = 0\n",
    "n_recovered = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(N_CELLS), desc='scGPT reconstruction'):\n",
    "\n",
    "        cell_expr = X_scgpt_in[i]          # shape (n_genes,); -1 for masked genes\n",
    "\n",
    "        # ── Select non-zero genes (includes -1 masked ones) ──────────────────\n",
    "        nonzero_idx = np.where(cell_expr != 0)[0]\n",
    "        if len(nonzero_idx) == 0:\n",
    "            continue\n",
    "\n",
    "        genes_sel  = gene_names[nonzero_idx]                    # gene name strings\n",
    "        vals_sel   = cell_expr[nonzero_idx].astype(np.float32)  # expr values (incl. -1)\n",
    "        gene_ids   = gene_names_to_ids(genes_sel)               # vocab IDs\n",
    "\n",
    "        # ── Build tensors ─────────────────────────────────────────────────────\n",
    "        src_t   = torch.from_numpy(gene_ids).unsqueeze(0).to(DEVICE)    # (1, L)\n",
    "        vals_t  = torch.from_numpy(vals_sel).unsqueeze(0).to(DEVICE)    # (1, L)\n",
    "        # padding mask: False = attend, True = ignore  (no padding here)\n",
    "        pad_mask = torch.zeros(1, len(gene_ids), dtype=torch.bool, device=DEVICE)\n",
    "\n",
    "        n_in_vocab += int((gene_ids != UNK_ID).sum())\n",
    "\n",
    "        # ── scGPT forward pass ────────────────────────────────────────────────\n",
    "        # mlm_output has shape (1, L) or (1, L, 1) – predicts expression at\n",
    "        # every position, including the masked (-1) ones.\n",
    "        out = scgpt_model(\n",
    "            src                  = src_t,\n",
    "            values               = vals_t,\n",
    "            src_key_padding_mask = pad_mask,\n",
    "            CLS                  = False,\n",
    "        )\n",
    "\n",
    "        mlm = out.get('mlm_output')\n",
    "        if mlm is None:\n",
    "            print(f'  Cell {i}: mlm_output not in scGPT output '\n",
    "                  f'(keys: {list(out.keys())}) – skipping')\n",
    "            continue\n",
    "\n",
    "        if mlm.ndim == 3:\n",
    "            mlm = mlm.squeeze(-1)           # (1, L, 1) → (1, L)\n",
    "        mlm_np = mlm.squeeze(0).cpu().float().numpy()   # (L,)\n",
    "\n",
    "        # ── Find masked positions and fill with scGPT predictions ─────────────\n",
    "        masked_in_local = np.where(vals_sel < 0)[0]    # positions where value was -1\n",
    "        if len(masked_in_local) == 0:\n",
    "            continue\n",
    "\n",
    "        masked_orig_idx = nonzero_idx[masked_in_local]  # back to global gene indices\n",
    "        predicted_vals  = np.clip(mlm_np[masked_in_local], 0.0, None)\n",
    "\n",
    "        X_reconstructed[i, masked_orig_idx] = predicted_vals\n",
    "        n_recovered += len(masked_orig_idx)\n",
    "\n",
    "print(f'\\nReconstruction complete.')\n",
    "print(f'  Avg genes in scGPT vocab / cell : {n_in_vocab / N_CELLS:.0f}')\n",
    "print(f'  Masked genes recovered          : {n_recovered} / {total_masked}'\n",
    "      f'  ({n_recovered / total_masked * 100:.1f} %)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-section5",
   "metadata": {},
   "source": [
    "## 5 · Build three AnnData objects\n",
    "\n",
    "| Name | Expression matrix | Description |\n",
    "|---|---|---|\n",
    "| `adata_original` | `X_orig` | Clean, unmodified cells |\n",
    "| `adata_masked` | `X_masked` | Corrupted: `MASK_FRACTION` genes zeroed |\n",
    "| `adata_reconstructed` | `X_reconstructed` | scGPT-repaired: masked genes filled with predicted expression |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-build-adatas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData objects created:\n",
      "  original         non-zero fraction: 0.047\n",
      "  masked           non-zero fraction: 0.028\n",
      "  reconstructed    non-zero fraction: 0.047\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "def make_adata(X_new, template_adata):\n",
    "    \"\"\"Clone the obs/var metadata from template and replace the expression matrix.\"\"\"\n",
    "    a = ad.AnnData(\n",
    "        X=sp.csr_matrix(X_new),\n",
    "        obs=template_adata.obs.copy(),\n",
    "        var=template_adata.var.copy(),\n",
    "    )\n",
    "    return a\n",
    "\n",
    "adata_original     = make_adata(X_orig,         adata_small)\n",
    "adata_masked       = make_adata(X_masked,        adata_small)\n",
    "adata_reconstructed = make_adata(X_reconstructed, adata_small)\n",
    "\n",
    "print('AnnData objects created:')\n",
    "for name, a in [('original', adata_original), ('masked', adata_masked), ('reconstructed', adata_reconstructed)]:\n",
    "    mean_nonzero = (a.X.toarray() > 0).mean()\n",
    "    print(f'  {name:15s}  non-zero fraction: {mean_nonzero:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-section6",
   "metadata": {},
   "source": [
    "## 6 · C2S Cell-Type Prediction (3×)\n",
    "\n",
    "> **Runtime note:** each C2S call takes ~9 min on CPU (22 s/cell × 24 cells).  \n",
    "> Total ≈ 27 min. Grab a coffee ☕"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-c2s-helpers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C2S model loaded: vandijklab/C2S-Pythia-410m-cell-type-prediction\n"
     ]
    }
   ],
   "source": [
    "label_cols = [c for c in ['cell_type', DONOR_COLUMN, 'tissue', 'sex', 'organism']\n",
    "              if c in adata_small.obs.columns]\n",
    "\n",
    "# Load C2S model once (we'll reuse it across all three predictions)\n",
    "csmodel = cs.CSModel(\n",
    "    model_name_or_path=C2S_MODEL,\n",
    "    save_dir='./tmp_c2s_reconstruction_model',\n",
    "    save_name='pretrained_c2s_inference',\n",
    ")\n",
    "print('C2S model loaded:', C2S_MODEL)\n",
    "\n",
    "\n",
    "def run_c2s(adata_in, tag, csmodel_in):\n",
    "    \"\"\"Run C2S cell-type prediction and return a DataFrame with y_true / y_pred.\"\"\"\n",
    "    print(f'\\n─── C2S: {tag} ───')\n",
    "    arrow_ds, vocab = cs.CSData.adata_to_arrow(\n",
    "        adata_in,\n",
    "        random_state=SEED,\n",
    "        sentence_delimiter=' ',\n",
    "        label_col_names=label_cols,\n",
    "    )\n",
    "    csdata = cs.CSData.csdata_from_arrow(\n",
    "        arrow_dataset=arrow_ds,\n",
    "        vocabulary=vocab,\n",
    "        save_dir=f'./tmp_c2s_{tag}',\n",
    "        save_name='data',\n",
    "        dataset_backend='arrow',\n",
    "    )\n",
    "    preds = predict_cell_types_of_data(\n",
    "        csdata=csdata,\n",
    "        csmodel=csmodel_in,\n",
    "        n_genes=TOP_K_GENES,\n",
    "        max_num_tokens=32,\n",
    "    )\n",
    "    return pd.DataFrame({\n",
    "        'cell_id': adata_in.obs_names.astype(str),\n",
    "        'y_true' : adata_in.obs['cell_type'].astype(str).values,\n",
    "        'y_pred' : [str(p).strip() for p in preds],\n",
    "        'version': tag,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cell-run-original",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "─── C2S: original ───\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN: more variables (36503) than observations (24)... did you mean to transpose the object (e.g. adata.T)?\n",
      "WARN: more variables (36503) than observations (24), did you mean to transpose the object (e.g. adata.T)?\n",
      "100%|██████████| 24/24 [00:00<00:00, 344.89it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 24/24 [00:00<00:00, 552.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model from path on disk: ./tmp_c2s_reconstruction_model\\pretrained_c2s_inference\n",
      "Predicting cell types for 24 cells using CSModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [08:47<00:00, 21.99s/it]\n"
     ]
    }
   ],
   "source": [
    "df_original = run_c2s(adata_original, 'original', csmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cell-run-masked",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN: more variables (36503) than observations (24)... did you mean to transpose the object (e.g. adata.T)?\n",
      "WARN: more variables (36503) than observations (24), did you mean to transpose the object (e.g. adata.T)?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "─── C2S: masked ───\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 1081.31it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 24/24 [00:00<00:00, 1477.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model from path on disk: ./tmp_c2s_reconstruction_model\\pretrained_c2s_inference\n",
      "Predicting cell types for 24 cells using CSModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [08:47<00:00, 21.98s/it]\n"
     ]
    }
   ],
   "source": [
    "df_masked = run_c2s(adata_masked, 'masked', csmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cell-run-reconstructed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN: more variables (36503) than observations (24)... did you mean to transpose the object (e.g. adata.T)?\n",
      "WARN: more variables (36503) than observations (24), did you mean to transpose the object (e.g. adata.T)?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "─── C2S: reconstructed ───\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:00<00:00, 797.41it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 24/24 [00:00<00:00, 2528.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading model from path on disk: ./tmp_c2s_reconstruction_model\\pretrained_c2s_inference\n",
      "Predicting cell types for 24 cells using CSModel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [08:36<00:00, 21.51s/it]\n"
     ]
    }
   ],
   "source": [
    "df_reconstructed = run_c2s(adata_reconstructed, 'reconstructed', csmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-section7",
   "metadata": {},
   "source": [
    "## 7 · 3-Tier Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cell-evaluate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring complete.\n"
     ]
    }
   ],
   "source": [
    "def normalize(text):\n",
    "    text = str(text).strip().rstrip('.').lower()\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def classify(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    2 = Exactly correct  (normalized match)\n",
    "    1 = Partly correct   (substring containment OR Jaccard word overlap ≥ 30%)\n",
    "    0 = Not correct\n",
    "    \"\"\"\n",
    "    t, p = normalize(y_true), normalize(y_pred)\n",
    "    if t == p:\n",
    "        return 2\n",
    "    if p in t or t in p:\n",
    "        return 1\n",
    "    t_w = {w for w in re.findall(r'\\b\\w+\\b', t) if len(w) > 2}\n",
    "    p_w = {w for w in re.findall(r'\\b\\w+\\b', p) if len(w) > 2}\n",
    "    if t_w and p_w and len(t_w & p_w) / len(t_w | p_w) >= 0.30:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "label_map = {2: 'Exactly correct', 1: 'Partly correct', 0: 'Not correct'}\n",
    "\n",
    "def score_df(df):\n",
    "    df = df.copy()\n",
    "    df['score']   = df.apply(lambda r: classify(r['y_true'], r['y_pred']), axis=1)\n",
    "    df['verdict'] = df['score'].map(label_map)\n",
    "    return df\n",
    "\n",
    "df_original     = score_df(df_original)\n",
    "df_masked       = score_df(df_masked)\n",
    "df_reconstructed = score_df(df_reconstructed)\n",
    "\n",
    "print('Scoring complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cell-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "  3-TIER ACCURACY COMPARISON\n",
      "=================================================================\n",
      "\n",
      "  Original (clean)\n",
      "    Exactly correct     :  4/24  ( 16.7%)  ███\n",
      "    Partly correct      : 13/24  ( 54.2%)  ██████████\n",
      "    Not correct         :  7/24  ( 29.2%)  █████\n",
      "\n",
      "  Masked (40% zeroed)\n",
      "    Exactly correct     :  8/24  ( 33.3%)  ██████\n",
      "    Partly correct      : 13/24  ( 54.2%)  ██████████\n",
      "    Not correct         :  3/24  ( 12.5%)  ██\n",
      "\n",
      "  Reconstructed (scGPT)\n",
      "    Exactly correct     :  1/24  (  4.2%)  \n",
      "    Partly correct      :  8/24  ( 33.3%)  ██████\n",
      "    Not correct         : 15/24  ( 62.5%)  ████████████\n",
      "=================================================================\n",
      "\n",
      "  Exact + Partial (score ≥ 1):\n",
      "    Original (clean)              : 17/24  (70.8%)\n",
      "    Masked (40% zeroed)           : 21/24  (87.5%)\n",
      "    Reconstructed (scGPT)         : 9/24  (37.5%)\n"
     ]
    }
   ],
   "source": [
    "def summary(df, name):\n",
    "    n = len(df)\n",
    "    rows = []\n",
    "    for s in [2, 1, 0]:\n",
    "        cnt = (df['score'] == s).sum()\n",
    "        rows.append({'Version': name, 'Tier': label_map[s],\n",
    "                     'Count': cnt, 'Pct': cnt / n * 100})\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "for df, name in [(df_original, 'Original (clean)'),\n",
    "                 (df_masked,   f'Masked ({int(MASK_FRACTION*100)}% zeroed)'),\n",
    "                 (df_reconstructed, 'Reconstructed (scGPT)')]:\n",
    "    rows.extend(summary(df, name))\n",
    "\n",
    "summary_df = pd.DataFrame(rows)\n",
    "\n",
    "print('=' * 65)\n",
    "print('  3-TIER ACCURACY COMPARISON')\n",
    "print('=' * 65)\n",
    "for name, grp in summary_df.groupby('Version', sort=False):\n",
    "    print(f'\\n  {name}')\n",
    "    for _, row in grp.iterrows():\n",
    "        bar = '█' * int(row['Pct'] / 5)\n",
    "        print(f'    {row[\"Tier\"]:<20}: {row[\"Count\"]:>2}/{N_CELLS}  ({row[\"Pct\"]:>5.1f}%)  {bar}')\n",
    "print('=' * 65)\n",
    "\n",
    "# Combined metric: Exact + Partial\n",
    "print('\\n  Exact + Partial (score ≥ 1):')\n",
    "for df, name in [(df_original, 'Original (clean)'),\n",
    "                 (df_masked,   f'Masked ({int(MASK_FRACTION*100)}% zeroed)'),\n",
    "                 (df_reconstructed, 'Reconstructed (scGPT)')]:\n",
    "    ep = (df['score'] >= 1).sum()\n",
    "    print(f'    {name:<30}: {ep}/{N_CELLS}  ({ep/N_CELLS*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cell-per-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT improved prediction: 1 cells\n",
      "scGPT hurt prediction    : 16 cells\n",
      "No change                : 7 cells\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_e3658_row16_col5 {\n",
       "  background-color: #c8e6c9;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_e3658\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e3658_level0_col0\" class=\"col_heading level0 col0\" >y_true</th>\n",
       "      <th id=\"T_e3658_level0_col1\" class=\"col_heading level0 col1\" >pred_masked</th>\n",
       "      <th id=\"T_e3658_level0_col2\" class=\"col_heading level0 col2\" >score_masked</th>\n",
       "      <th id=\"T_e3658_level0_col3\" class=\"col_heading level0 col3\" >pred_reconstructed</th>\n",
       "      <th id=\"T_e3658_level0_col4\" class=\"col_heading level0 col4\" >score_reconstructed</th>\n",
       "      <th id=\"T_e3658_level0_col5\" class=\"col_heading level0 col5\" >scgpt_helped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e3658_row0_col0\" class=\"data row0 col0\" >mast cell</td>\n",
       "      <td id=\"T_e3658_row0_col1\" class=\"data row0 col1\" >mast cell.</td>\n",
       "      <td id=\"T_e3658_row0_col2\" class=\"data row0 col2\" >2</td>\n",
       "      <td id=\"T_e3658_row0_col3\" class=\"data row0 col3\" >CD4-positive, alpha-beta thymocyte.</td>\n",
       "      <td id=\"T_e3658_row0_col4\" class=\"data row0 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row0_col5\" class=\"data row0 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e3658_row1_col0\" class=\"data row1 col0\" >T follicular helper cell</td>\n",
       "      <td id=\"T_e3658_row1_col1\" class=\"data row1 col1\" >CD4-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row1_col2\" class=\"data row1 col2\" >0</td>\n",
       "      <td id=\"T_e3658_row1_col3\" class=\"data row1 col3\" >CD4-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row1_col4\" class=\"data row1 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row1_col5\" class=\"data row1 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e3658_row2_col0\" class=\"data row2 col0\" >plasma cell</td>\n",
       "      <td id=\"T_e3658_row2_col1\" class=\"data row2 col1\" >plasma cell.</td>\n",
       "      <td id=\"T_e3658_row2_col2\" class=\"data row2 col2\" >2</td>\n",
       "      <td id=\"T_e3658_row2_col3\" class=\"data row2 col3\" >malignant cell.</td>\n",
       "      <td id=\"T_e3658_row2_col4\" class=\"data row2 col4\" >1</td>\n",
       "      <td id=\"T_e3658_row2_col5\" class=\"data row2 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e3658_row3_col0\" class=\"data row3 col0\" >plasma cell</td>\n",
       "      <td id=\"T_e3658_row3_col1\" class=\"data row3 col1\" >plasma cell.</td>\n",
       "      <td id=\"T_e3658_row3_col2\" class=\"data row3 col2\" >2</td>\n",
       "      <td id=\"T_e3658_row3_col3\" class=\"data row3 col3\" >germ cell.</td>\n",
       "      <td id=\"T_e3658_row3_col4\" class=\"data row3 col4\" >1</td>\n",
       "      <td id=\"T_e3658_row3_col5\" class=\"data row3 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e3658_row4_col0\" class=\"data row4 col0\" >effector memory CD4-positive, alpha-beta T cell</td>\n",
       "      <td id=\"T_e3658_row4_col1\" class=\"data row4 col1\" >naive thymus-derived CD4-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row4_col2\" class=\"data row4 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row4_col3\" class=\"data row4 col3\" >germ cell.</td>\n",
       "      <td id=\"T_e3658_row4_col4\" class=\"data row4 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row4_col5\" class=\"data row4 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e3658_row5_col0\" class=\"data row5 col0\" >naive thymus-derived CD4-positive, alpha-beta T cell</td>\n",
       "      <td id=\"T_e3658_row5_col1\" class=\"data row5 col1\" >CD4-positive, alpha-beta memory T cell.</td>\n",
       "      <td id=\"T_e3658_row5_col2\" class=\"data row5 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row5_col3\" class=\"data row5 col3\" >germ cell.</td>\n",
       "      <td id=\"T_e3658_row5_col4\" class=\"data row5 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row5_col5\" class=\"data row5 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e3658_row6_col0\" class=\"data row6 col0\" >macrophage</td>\n",
       "      <td id=\"T_e3658_row6_col1\" class=\"data row6 col1\" >macrophage.</td>\n",
       "      <td id=\"T_e3658_row6_col2\" class=\"data row6 col2\" >2</td>\n",
       "      <td id=\"T_e3658_row6_col3\" class=\"data row6 col3\" >erythroid lineage cell.</td>\n",
       "      <td id=\"T_e3658_row6_col4\" class=\"data row6 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row6_col5\" class=\"data row6 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e3658_row7_col0\" class=\"data row7 col0\" >memory B cell</td>\n",
       "      <td id=\"T_e3658_row7_col1\" class=\"data row7 col1\" >naive B cell.</td>\n",
       "      <td id=\"T_e3658_row7_col2\" class=\"data row7 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row7_col3\" class=\"data row7 col3\" >germ cell.</td>\n",
       "      <td id=\"T_e3658_row7_col4\" class=\"data row7 col4\" >1</td>\n",
       "      <td id=\"T_e3658_row7_col5\" class=\"data row7 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e3658_row8_col0\" class=\"data row8 col0\" >memory B cell</td>\n",
       "      <td id=\"T_e3658_row8_col1\" class=\"data row8 col1\" >B cell.</td>\n",
       "      <td id=\"T_e3658_row8_col2\" class=\"data row8 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row8_col3\" class=\"data row8 col3\" >malignant cell.</td>\n",
       "      <td id=\"T_e3658_row8_col4\" class=\"data row8 col4\" >1</td>\n",
       "      <td id=\"T_e3658_row8_col5\" class=\"data row8 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e3658_row9_col0\" class=\"data row9 col0\" >erythroid lineage cell</td>\n",
       "      <td id=\"T_e3658_row9_col1\" class=\"data row9 col1\" >erythroid progenitor cell.</td>\n",
       "      <td id=\"T_e3658_row9_col2\" class=\"data row9 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row9_col3\" class=\"data row9 col3\" >germ cell.</td>\n",
       "      <td id=\"T_e3658_row9_col4\" class=\"data row9 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row9_col5\" class=\"data row9 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_e3658_row10_col0\" class=\"data row10 col0\" >alveolar macrophage</td>\n",
       "      <td id=\"T_e3658_row10_col1\" class=\"data row10 col1\" >macrophage.</td>\n",
       "      <td id=\"T_e3658_row10_col2\" class=\"data row10 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row10_col3\" class=\"data row10 col3\" >malignant cell.</td>\n",
       "      <td id=\"T_e3658_row10_col4\" class=\"data row10 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row10_col5\" class=\"data row10 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_e3658_row11_col0\" class=\"data row11 col0\" >CD16-negative, CD56-bright natural killer cell, human</td>\n",
       "      <td id=\"T_e3658_row11_col1\" class=\"data row11 col1\" >CD8-positive, alpha-beta memory T cell.</td>\n",
       "      <td id=\"T_e3658_row11_col2\" class=\"data row11 col2\" >0</td>\n",
       "      <td id=\"T_e3658_row11_col3\" class=\"data row11 col3\" >CD8-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row11_col4\" class=\"data row11 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row11_col5\" class=\"data row11 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_e3658_row12_col0\" class=\"data row12 col0\" >classical monocyte</td>\n",
       "      <td id=\"T_e3658_row12_col1\" class=\"data row12 col1\" >classical monocyte.</td>\n",
       "      <td id=\"T_e3658_row12_col2\" class=\"data row12 col2\" >2</td>\n",
       "      <td id=\"T_e3658_row12_col3\" class=\"data row12 col3\" >epithelial cell.</td>\n",
       "      <td id=\"T_e3658_row12_col4\" class=\"data row12 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row12_col5\" class=\"data row12 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_e3658_row13_col0\" class=\"data row13 col0\" >classical monocyte</td>\n",
       "      <td id=\"T_e3658_row13_col1\" class=\"data row13 col1\" >monocyte.</td>\n",
       "      <td id=\"T_e3658_row13_col2\" class=\"data row13 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row13_col3\" class=\"data row13 col3\" >epithelial cell.</td>\n",
       "      <td id=\"T_e3658_row13_col4\" class=\"data row13 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row13_col5\" class=\"data row13 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_e3658_row14_col0\" class=\"data row14 col0\" >naive thymus-derived CD4-positive, alpha-beta T cell</td>\n",
       "      <td id=\"T_e3658_row14_col1\" class=\"data row14 col1\" >naive thymus-derived CD4-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row14_col2\" class=\"data row14 col2\" >2</td>\n",
       "      <td id=\"T_e3658_row14_col3\" class=\"data row14 col3\" >CD4-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row14_col4\" class=\"data row14 col4\" >1</td>\n",
       "      <td id=\"T_e3658_row14_col5\" class=\"data row14 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_e3658_row15_col0\" class=\"data row15 col0\" >effector memory CD8-positive, alpha-beta T cell, terminally differentiated</td>\n",
       "      <td id=\"T_e3658_row15_col1\" class=\"data row15 col1\" >CD8-positive, alpha-beta memory T cell.</td>\n",
       "      <td id=\"T_e3658_row15_col2\" class=\"data row15 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row15_col3\" class=\"data row15 col3\" >CD8-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row15_col4\" class=\"data row15 col4\" >1</td>\n",
       "      <td id=\"T_e3658_row15_col5\" class=\"data row15 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_e3658_row16_col0\" class=\"data row16 col0\" >CD4-positive helper T cell</td>\n",
       "      <td id=\"T_e3658_row16_col1\" class=\"data row16 col1\" >mature alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row16_col2\" class=\"data row16 col2\" >0</td>\n",
       "      <td id=\"T_e3658_row16_col3\" class=\"data row16 col3\" >naive thymus-derived CD4-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row16_col4\" class=\"data row16 col4\" >1</td>\n",
       "      <td id=\"T_e3658_row16_col5\" class=\"data row16 col5\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_e3658_row17_col0\" class=\"data row17 col0\" >T follicular helper cell</td>\n",
       "      <td id=\"T_e3658_row17_col1\" class=\"data row17 col1\" >CD4-positive helper T cell.</td>\n",
       "      <td id=\"T_e3658_row17_col2\" class=\"data row17 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row17_col3\" class=\"data row17 col3\" >thymocyte.</td>\n",
       "      <td id=\"T_e3658_row17_col4\" class=\"data row17 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row17_col5\" class=\"data row17 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_e3658_row18_col0\" class=\"data row18 col0\" >naive thymus-derived CD4-positive, alpha-beta T cell</td>\n",
       "      <td id=\"T_e3658_row18_col1\" class=\"data row18 col1\" >CD4-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row18_col2\" class=\"data row18 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row18_col3\" class=\"data row18 col3\" >germ cell.</td>\n",
       "      <td id=\"T_e3658_row18_col4\" class=\"data row18 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row18_col5\" class=\"data row18 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_e3658_row19_col0\" class=\"data row19 col0\" >naive B cell</td>\n",
       "      <td id=\"T_e3658_row19_col1\" class=\"data row19 col1\" >B cell.</td>\n",
       "      <td id=\"T_e3658_row19_col2\" class=\"data row19 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row19_col3\" class=\"data row19 col3\" >naive thymus-derived CD4-positive, alpha-beta T cell.</td>\n",
       "      <td id=\"T_e3658_row19_col4\" class=\"data row19 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row19_col5\" class=\"data row19 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_e3658_row20_col0\" class=\"data row20 col0\" >macrophage</td>\n",
       "      <td id=\"T_e3658_row20_col1\" class=\"data row20 col1\" >macrophage.</td>\n",
       "      <td id=\"T_e3658_row20_col2\" class=\"data row20 col2\" >2</td>\n",
       "      <td id=\"T_e3658_row20_col3\" class=\"data row20 col3\" >malignant cell.</td>\n",
       "      <td id=\"T_e3658_row20_col4\" class=\"data row20 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row20_col5\" class=\"data row20 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_e3658_row21_col0\" class=\"data row21 col0\" >macrophage</td>\n",
       "      <td id=\"T_e3658_row21_col1\" class=\"data row21 col1\" >elicited macrophage.</td>\n",
       "      <td id=\"T_e3658_row21_col2\" class=\"data row21 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row21_col3\" class=\"data row21 col3\" >CD4-positive helper T cell.</td>\n",
       "      <td id=\"T_e3658_row21_col4\" class=\"data row21 col4\" >0</td>\n",
       "      <td id=\"T_e3658_row21_col5\" class=\"data row21 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_e3658_row22_col0\" class=\"data row22 col0\" >macrophage</td>\n",
       "      <td id=\"T_e3658_row22_col1\" class=\"data row22 col1\" >macrophage.</td>\n",
       "      <td id=\"T_e3658_row22_col2\" class=\"data row22 col2\" >2</td>\n",
       "      <td id=\"T_e3658_row22_col3\" class=\"data row22 col3\" >macrophage.</td>\n",
       "      <td id=\"T_e3658_row22_col4\" class=\"data row22 col4\" >2</td>\n",
       "      <td id=\"T_e3658_row22_col5\" class=\"data row22 col5\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e3658_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_e3658_row23_col0\" class=\"data row23 col0\" >memory B cell</td>\n",
       "      <td id=\"T_e3658_row23_col1\" class=\"data row23 col1\" >naive B cell.</td>\n",
       "      <td id=\"T_e3658_row23_col2\" class=\"data row23 col2\" >1</td>\n",
       "      <td id=\"T_e3658_row23_col3\" class=\"data row23 col3\" >germ cell.</td>\n",
       "      <td id=\"T_e3658_row23_col4\" class=\"data row23 col4\" >1</td>\n",
       "      <td id=\"T_e3658_row23_col5\" class=\"data row23 col5\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x26b456b2f90>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ── Per-cell side-by-side view ────────────────────────────────────────────────\n",
    "compare = df_original[['cell_id', 'y_true']].copy()\n",
    "compare['pred_original']     = df_original['y_pred']\n",
    "compare['score_original']    = df_original['score']\n",
    "compare['pred_masked']       = df_masked['y_pred']\n",
    "compare['score_masked']      = df_masked['score']\n",
    "compare['pred_reconstructed']  = df_reconstructed['y_pred']\n",
    "compare['score_reconstructed'] = df_reconstructed['score']\n",
    "compare['scgpt_helped'] = compare['score_reconstructed'] > compare['score_masked']\n",
    "compare['scgpt_hurt']   = compare['score_reconstructed'] < compare['score_masked']\n",
    "\n",
    "print(f'scGPT improved prediction: {compare[\"scgpt_helped\"].sum()} cells')\n",
    "print(f'scGPT hurt prediction    : {compare[\"scgpt_hurt\"].sum()} cells')\n",
    "print(f'No change                : {(~compare[\"scgpt_helped\"] & ~compare[\"scgpt_hurt\"]).sum()} cells')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 55)\n",
    "compare[['y_true',\n",
    "         'pred_masked', 'score_masked',\n",
    "         'pred_reconstructed', 'score_reconstructed',\n",
    "         'scgpt_helped']].style.apply(\n",
    "    lambda col: ['background-color: #c8e6c9' if v else\n",
    "                 'background-color: #ffcdd2' if col.name == 'scgpt_hurt' else ''\n",
    "                 for v in col], subset=['scgpt_helped']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cell-save",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Daniel\\Desktop\\GitProjects\\Improving-Cell2Sentence-with-Single-Cell-Foundation-Model-Embeddings\\notebooks\\c2s_donor_new_approach\\scgpt_reconstruction_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ── Save full results ─────────────────────────────────────────────────────────\n",
    "out = Path('./scgpt_reconstruction_results.csv')\n",
    "compare.to_csv(out, index=False)\n",
    "print('Saved:', out.resolve())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c2s_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
